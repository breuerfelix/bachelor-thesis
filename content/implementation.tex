\section{Architektur und Implementation}
In Kapitel \ref{s:solution} wurden einige Konzepte und deren Implementation für einen \textit{Smart Load Balancer} für \ac{mqtt} beschrieben. In diesem Kapitel wird gezeigt, wie man viele dieser einzelnen Konzepte in einen Envoy Load Balancer integriert und verwendet.
Im Rahmen der Thesis wurde die \ac{dns} Cluster Discovery \ref{ss:dns-discovery}, das weighted \ac{cpu} Round Robin \ref{ss:weighted-cpu}, das Circuit Breaking \ref{ss:circuit-breaking} und das \ac{mqtt} Health Checking \ref{ss:health-check} aus Kapitel \ref{s:solution} in einer Envoy Control Plane implementiert.

% TODO figure con der architecture
\subsection{Envoy Control Plane} \label{si:control-plane}
Die Envoy Control Plane ist das Herzstück des Smart Load Balancers. Die einzelnen Envoy Instanzen erhalten alle nötigen Informationen dynamisch von der Control Plane und kriegen statisch nur die Information wie die Control Plane zu erreichen ist provisioniert.
Die Konfigurationen werden mit gRPC zwischen Envoy und Control Plane ausgetauscht. Daher muss die Control Plane einen gRPC Server implementieren, der von allen Envoys erreichbar ist.
Als Quellcodebasis für die Control Plane wurde ein minimales Beispiel aus dem offiziellen Github Repository \cite{EnvoyproxyGocontrolplane} verwendet. Dieses erzeugt eine Envoy Konfiguration die einen \ac{http} Endpunkt definiert. Da die offizielle Referenzimplementation einer Control Plane in Golang programmiert ist, wurde die Control Plane des Smart Load Balancers, nachfolgend Control Plane genannt, ebenfalls in Golang implementiert.
\\
Die parallele Ausführung von mehreren Go Routinen ist ein wichtiger Bestandteil der Control Plane. Diese muss asynchron und nicht blockend Aufgaben abarbeiten wie zum Beispiel das Überprüfen eines Health Status der HiveMQ Nodes und parallel den neusten Snapshot an alle Envoys ausliefern.
Um bei einem asynchronen Programm keine \textit{Data Races} zu verursachen, muss darauf geachtet werden, dass immer nur eine Go Routine Schreibzugriff auf eine Variable erhält und lesende Go Routinen sich zu Beginn eine Kopie der Variable erstellen, damit diese nicht während der Bearbeitung von der schreibenden Go Routine verändert wird.
\\
Die Control Plane besteht aus folgenden Komponenten:% TODO komponenten diagram zeigen
\begin{itemize}
  \item \textbf{Server:} Der Server akzeptiert eingehende gRPC Verbindungen von Envoys. Sobald eine neue Snapshot Version im Cache vorliegt, wird diese an alle Envoys mit einer älteren Version ausgeliefert.
  \item \textbf{Cluster:} Ein Cluster repräsentiert ein HiveMQ Cluster und dient als Container für eine Liste von allen HiveMQ Node Strukturen. Ein Cluster enthält zudem einen Domain Namen unter dem die \ac{dns} Discovery die einzelnen Nodes entdecken kann.
  \item \textbf{HiveMQ:} HiveMQ ist eine Struktur die alle Daten eines HiveMQ Nodes enthält. Darunter befinden sich \ac{ip} Adresse, Port, Health Status, Weight und nach Zeitstempel sortierte Metriken für einen bestimmten Zeitraum.
  \item \textbf{Resources:} Generiert Envoy Ressourcen basierend einer Cluster Struktur. Es wird ein \ac{tcp} Listener und ein Upsteam Envoy Cluster mit allen Endpunkten und Gewichtungen erzeugt. Das Endprodukt ist ein Snapshot, das in den Cache gelegt wird.
  \item \textbf{Metrics:} Fragt periodisch alle Metriken der HiveMQ Nodes einer Cluster Struktur ab. Basierend eines gegebenen Intervalls werden die Metriken verarbeitet und Health Status sowieso Gewichtung berechnet.
\end{itemize}

% TODO classes UML Diagram
\paragraph{Server}
In der \verb|main| Methode der Control Plane wird zu Beginn ein Cache Struct erstellt und die Referenz dem Server übergeben, der in der Main Go Routine ausgeführt wird. Dieser empfängt eingehende Envoy Verbindungen und liefert den neusten Snapshot aus dem Cache aus. Da eine Referenz des Caches übergeben wurde, können parallel andere Go Routinen ebenfalls auf den Cache zugreifen und neue Snapshots einfügen. Dem Server wird au{\ss}erdem ein Endpunkt und Port übergeben auf dem dieser einen \ac{tcp} Listener für die Envoy Instanzen registriert.

\paragraph{Cluster}
In der \verb|main| Methode der Control Plane wird eine Cluster Struktur erstellt und die Referenz in einer Variable gespeichert. Diese Referenz wird folgend an alle anderen Go Routinen übergeben, damit immer auf die aktuelle HiveMQ Cluster Topologie zugegriffen werden kann. Die Liste der HiveMQ Nodes in einer Cluster Struktur ist ein Golang Slice. Die Elemente dieses Slices werden niemals geändert. Falls sich die Topologie des HiveMQ Clusters ändert, wird ein neues Slice erstellt und in der Cluster Struktur referenziert. Sobald sich eine Go Routine das Slice eines Clusters in einer Variable speichert ist dadurch garantiert, dass sich die Werte dieses Slices nicht mehr ändern. Dies ist erforderlich, da es in Golang keine unveränderbare Listen gibt.

\paragraph{HiveMQ}
Eine HiveMQ Struktur repräsentiert ein HiveMQ Node und enthält alle Metadaten eines HiveMQ Nodes. Die Funktion \verb|NewHiveMQ| erstellt eine neue HiveMQ Struktur und gibt die Referenz auf diese zurück. Von HiveMQ Strukturen existieren nur Referenzen in der Control Plane. Dadurch können Werte der einzelnen Strukturen verändert werden ohne eine neue Struktur zu erstellen.
Eine HiveMQ Struktur besitzt eine Map mit einem String als Schlüssel und einer Liste aus Datenpunkten als Wert. In dieser Map werden die Metriken des HiveMQ Nodes abgespeichert. Der Schlüssel ist der jeweilige Prometheus Schlüssel der Metrik und die Liste der Datenpunkte ist nach Zeitstempel sortiert. Der erste Wert der Liste ist der älteste Wert. Damit die Control Plane nicht unendlich viel Arbeitsspeicher verbraucht indem sie alle Metriken für die gesamte Laufzeit abspeichert, werden Metriken, die älter als 60 Sekunden sind, gelöscht.

\paragraph{Resources}
Die Funktion \verb|GenerateSnapshot| aus dem Resources Namespace kriegt eine Liste von HiveMQ Strukturen übergeben. Aus dieser Liste wird in dieser Komponente die gesamte Envoy Konfiguration erstellt. Zunächst wird die Liste der Nodes nach ihrer ID sortiert damit eine Veränderung der Reihenfolge der Nodes keine Veränderung der Snapshotversion zur Folge hat. Nachdem alle Ressourcen entsprechend der HiveMQ Strukturdaten generiert wurden, wird eine Snapshot Version erstellt. Diese ist der Hash von \ac{ip} Adresse, Port, Health Status und Gewichtung eines jeden HiveMQ Nodes. Durch den Hash wird sichergestellt, dass eine Änderung von einer dieser Werte eine neue Version zur Folge hat. Au{\ss}erdem werden keine neuen Versionen erstellt, falls sich die Daten nicht ändern wodurch keine vermeintlich neuen Version an die Envoys ausgeliefert werden.

\paragraph{Metrics}
Die Metriken Komponente ruft bei Aufruf der \verb|InitMetrics| Funktion die aktuellen Metriken aller HiveMQ Nodes ab und startet zwei Go Routinen. Durch den initialen Abruf der Metriken werden alle Variablen initialisiert. Eine der beiden Go Routinen ist für das periodische Abrufen aller Metriken zuständig. Diese werden in der HiveMQ Struktur gespeichert. Die andere Go Routine verarbeitet periodisch die abgerufenen Metriken und berechnet die aktuelle Gewichtung und den Health Status. Diese Jobs wurden in zwei Go Routinen aufgeteilt um öfter die Metriken abzurufen als diese zu verarbeiten. Dies führt zu einer höheren Auflösung beim verarbeiten der Daten ohne alle zwei Sekunden die Envoy Konfiguration zu aktualisieren.

\paragraph{Main}
Die Main Komponente ist der Eintiegspunkt der Control Plane. Bei Aufruf wird zunächst eine Cluster Struktur erstellt, eine gegebene Domain aufgelöst und für alle \ac{ip} Adressen eine HiveMQ Struktur erstellt.
Anschlie{\ss}end werden zwei Go Routinen gestartet. Die erste löst periodisch den Domain Namen des Clusters auf um Topologieänderungen am Cluster umzusetzen. Die zweite Go Routine prüft periodisch die \ac{mqtt} Funktionalität eines jeden HiveMQ Nodes durch die Methode \verb|CheckHealth| der HiveMQ Struktur.

\subsubsection{DNS Cluster Discovery}
Die \ac{dns} Cluster Discovery aus Kapitel \ref{ss:dns-discovery} ist in der Main Komponente implementiert. Quellcodeauszug \ref{code:discover-cluster} zeigt die Funktion \verb|discoverCluster|, die in der Main Funktion als Go Routine asynchron aufgerufen wird.
In Zeile 4 wird eine neue Liste erstellt, das in Zeile 25 in die Variable \verb|cluster.Nodes| gespeichert wird. Wie in Kapitel \ref{si:control-plane} erwähnt wird somit immer eine neue Liste erstellt anstatt die vorhandene zu editieren.
Falls ein HiveMQ Node bereits existiert wird die Referenz der HiveMQ Struktur in Zeile 10 in die neue Liste übernommen.
Zeile 21 setzt vorraus, dass alle HiveMQ Nodes unter dem Standard Port 1883 erreichbar sind.
\begin{figure}
    \import{gen/}{discover-cluster}
    \caption{Go Routine die periodisch eine Domain auflöst und HiveMQ Strukturen erstellt}
    \label{code:discover-cluster}
\end{figure}

\subsubsection{Weighted Round Robin}
Der Weighted Round Robin Algorithmus basierend auf der \ac{cpu} Auslastung der einzelnen HiveMQ Nodes aus Kapitel \ref{ss:weighted-cpu} ist in der Metrics Komponente implementiert.
\begin{figure}
    \import{gen/}{weighted-cpu}
    \caption{Algorithmus um die Gewichtung der HiveMQ Nodes basierend der CPU Auslastung zu bestimmen}
    \label{code:weighted-cpu}
\end{figure}

\subsubsection{Health Check} \label{si:health-check}
Der \ac{mqtt} Health Check aus Kapitel \ref{ss:health-check} ist in der Main und HiveMQ Komponente implementiert. In der HiveMQ Komponente wird die Funktion \verb|chechHealth| aus Quellcodeauszug \ref{code:sample-mqtt-client} eingefügt. Diese wird periodisch für jeden HiveMQ Node in einer Go Routine der Main Komponente ausgeführt. Wenn die \verb|checkHealth| Methode \verb|false| zurückgibt, ist der HiveMQ Node nicht mehr in der Lage neue Client Verbindungen zu erhalten und sein Health Status wird auf \verb|UNHEALTHY| gesetzt. Es wird au{\ss}erdem sofort ein neuer Snapshot in den Cache geschrieben, damit so wenig Clients wie möglich mit einen \verb|UNHEALTHY| HiveMQ Node verbunden werden. Der \ac{mqtt} Health Status wird au{\ss}erdem in die Variable \verb|MqttHealthy| der HiveMQ Struktur gespeichert damit diese von anderen Komponenten verarbeitet werden kann. Für den Fall, dass mehrere Faktoren den Health Status eines Nodes bestimmen, müssen alle Faktoren den Status \verb|HEALTHY| aufweisen damit ein Node derart gekennzeichnet wird.

\subsubsection{Circuit Breaking}
Das Circuit Breaking aus Kapitel \ref{ss:circuit-breaking} ist in der Metrics Komponente implementiert. Quellcodeauszug \ref{code:circuit-breaking} zeigt den Algorithmus um den Health Status aller Nodes basierend der Metrik \verb|OverloadProtection| zu bestimmen. In Zeile 10 wird auf den zuletzt bekannten Wert der Metrik zugegriffen und in Zeile 12 wird die Varible \verb|OverloadProtectionHealthy| der HiveMQ Struktur auf \verb|true| gesetzt, falls der Wert der Metrik kleiner oder gleich 5 ist.
Sobald der Overload Protection Wert überschritten wurde oder der \ac{mqtt} Health Check aus Kapitel \ref{si:health-check} fehlgeschlagen ist, darf der HiveMQ Node nicht mehr als \verb|HEALTHY| gekennzeichnet werden. Zeile 18 - 22 setzen den Health Status erst auf \verb|HEALTHY| sobald beide Bedingungen erfüllt sind.
\begin{figure}
    \import{gen/}{circuit-breaking}
    \caption{Algorithmus der den Health Status eines HiveMQ Nodes basierend der Overload Protection Metrik bestimmt}
    \label{code:circuit-breaking}
\end{figure}
\newpage

\subsection{Deployment}
\subsubsection{HiveMQ}
\subsubsection{Control-Plane}
\subsubsection{Envoy}
\subsubsection{Test Szenarien}
\newpage

\begin{comment}
- Describe the realization of your concepts, in case you have actually developed some-thing.
- Elaborate on the software architecture of your tool, in case you have developed one. Use nested UML packages, components, and interfaces in a component diagram.
- If applicable, show the deploymentof your tool in a production environment. Use UML's deployment diagram notation.
- It must be clear from the architecture, what your thesis contributes and what it takes for granted like existing systems, code bases, libraries and frameworks. For example, you can decorate the components in a UML component diagram that you have implemented and those that you just used.
- Do not delve into ordinary details by, e.g., intensively describing a "p lain old Java-object (POJO)" in all its dreary getter-setter-details. Instead pick some interesting details and de-scribe them, e.g.,
  - if   you made extensive use of a certain design pattern, describe a single concrete appli-cation of it using, e.g., UML class diagrams or
  - if your work involves a complicated conversation pattern or protocol, explained it using a UML sequence diagram, state chart, activity diagram and the like or
  - if you have developed a central and canny algorithm, you may even show its implemen-tation in, e.g., Java code.
- Show how the result of your work actually looks like. In case of a tool, provide some screenshots together with some explanatory text.
- Describe the quantity of your work, e.g., in terms of lines of code or classes etc. Please just count your own hand-crafted code but not previously existing, imported or generated code.
- Describe the quality of your work, e.g., if you have developed a large web application, run some performance tests, depicts results and draw conclusions from them.
\end{comment}
