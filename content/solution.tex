\section{Lösungskonzept}
%nur das konzept WIE ich das lösen will zb. man kann wie clientid parsen mit einem WASM modul
%in der nächsten section zeige ich dann den eigentlichen code welcher dies tut

\subsection{Cluster Discovery} \label{ss:cluster-discovery}
% was ist das ? hivemq extension! what is ttl? health checks? try a connect and disconnect as health check?

HiveMQ hat mehrere Mechanismen um die individuellen Nodes zu entdecken, die in das Cluster aufgenommen werden sollen. Ein \ac{lb} muss in der Lage sein die Nodes des Clusters mit dem selben Mechanismus wie HiveMQ ausfindig zu machen. Wenn der \ac{lb} einen anderen Mechanismus benutzen würde, dann könnte der \ac{lb} eine andere Cluster Topologie als der HiveMQ Cluster bilden.\\
Envoy hat bereits mehrere Möglichkeiten Hosts eines Clusters zu entdecken:
\begin{itemize}
  \item \textbf{Static:} Alle Nodes eines Clusters werden statisch in die Envoy Konfiguration eingetragen.
  \item \textbf{Strict \ac{dns}:} Envoy löst periodisch und asynchron einen konfigurierten \ac{dns} Namen auf. Jede eingetragene \ac{ip} Adresse wird zu einem Node des Clusters. Falls ein Node entfernt wurde, werden keine neuen Clients mehr mit diesem Node verbunden werden. Mit der Variable \verb|dns_refresh_rate| kann die Frequenz, in welcher der \ac{dns} Eintrag abgefragt wird, bestimmt werden.
  \item \textbf{Logical \ac{dns}:} Ähnlich wie bei Strict \ac{dns} löst Envoy einen konfigurierten \ac{dns} Namen auf. Bei jeder neuen eingehenden Verbindung wird der \ac{dns} Name erneut aufgelöst und die erste \ac{ip} Adresse als Ziel der neuen Verbindung genommen.
  \item \textbf{Original Destination:} Envoy leitet eingehende Verbindungen anhand der \textit{Redirect Metadata} weiter. Eingehende Verbindungen müssen dafür mit einem \textit{iptables REDIRECT}, \textit{TPROXY target} oder \textit{Proxy Protocol} an Envoy weitergeleitet werden.
  \item \textbf{Endpoint Discovery Service:} Envoy ruft die Nodes eines Cluster bei einem \textit{xDS Management Server} ab. Es werden Java und Golang Bibliotheken angeboten um einen Management Server für Envoy zu programmieren und bereitzustellen. Somit ist es möglich eine komplexe Service Discovery zu implementieren.
\end{itemize}
\cite{ServiceDiscoveryEnvoy}
In Kapitel \ref{s:hivemq-cluster} wurden folgende Methoden der HiveMQ Cluster Discovery erläutert:
\begin{itemize}
  \item static
  \item multicast
  \item broadcast
  \item extension
  \item dns extension
\end{itemize}
Envoy und HiveMQ stellen beide eine statische Cluster Discovery zur Verfügung. Diese Methode ist für Cloud oder Container Umgebungen nicht optimal. Sie bietet keine Möglichkeit die Cluster Topologie dynamisch zu verändern. Container Management Umgebungen wie zum Beispiel Kubernetes erlauben dynamische Vorgänge wie das Skalieren der Replika-Sets oder Rolling-Updates. Eine statische Cluster Konfiguration schlie{\ss}t diese oder ähnliche dynamische Vorgänge aus.\\
% TODO cite dynamische vorgänge in k8s
Eine weitere gemeinsame Cluster Discovery Methode ist die Strict \ac{dns} Methode. HiveMQ hat diese Methode nicht in der Standard Version eingebaut, stellt für diesen Anwendungsfall aber eine frei zugängliche Erweiterung bereit.
Bei dieser Methode werden periodisch alle Einträge zu einem gegebenen \ac{dns} Eintrag abgefragt. Alle erhaltenen Einträge werden als Nodes des Cluster anerkannt. Die Frequenz der Abfrage kann in Envoy mit der Variable \verb|dns_refresh_rate| bestimmt werden. In der HiveMQ Erweiterung ist die Einstellung der Frequenz derzeit noch nicht möglich. Ein Issue \cite{AllowConfigurationDiscovery} und ein Pull Request \cite{ExponentialBackoffGeneral} wurden bereits zu diesem Feature auf GitHub erstellt.
\\
TODO beispiel DIG command zeigen mit ip auflösung
\\
Der Quellcodeauszug \ref{code:envoy-strict-dns} zeigt eine Envoy Cluster Konfiguration, die den \ac{dns} namen \verb|example.cluster| auflöst und neue Verbindungen auf alle Einträge an den Port \verb|1883| verteilt.
\begin{figure}
    \import{gen/}{envoy-strict-dns}
    \caption{Envoy Strict \ac{dns} Konfiguration}
    \label{code:envoy-strict-dns}
\end{figure}
Die \textit{HiveMQ DNS Cluster Discovery Extension} \cite{HiveMQExtensionDNS} muss auf allen Nodes in den Ordner \verb|/opt/hivemq/extensions| kopiert werden. Der Quellcodeauszug \ref{code:hivemq-dnsdiscovery} zeigt eine Konfigurationsdatei, die im Pfad \verb|/opt/hivemq/conf/dnsdiscovery.properties| liegen muss und das Cluster aus allen Einträgen des \ac{dns} Namens \verb|example.cluster| bildet.
\begin{figure}
    \import{gen/}{dnsdiscovery}
    \caption{HiveMQ \ac{dns} Cluster Discovery Konfiguration}
    \label{code:hivemq-dnsdiscovery}
\end{figure}

\subsection{Lastverteilung HiveMQ Cluster}

In Kapitel \ref{sp:load} wurde erläutert, dass in einem HiveMQ Cluster eine ungleiche Lastverteilung durch die verschiedenen Verhaltensweisen der Clients auftreten kann.
In dem Fall, dass ein Node eine höhere Belastung als ein anderer Node aufweist, möchte man die Gewichtung der Verteilung aller neuer Clients an die Situation anpassen. Es sollen mehr Clients an den Node mit geringerer Belastung Verbunden werden.
Einen Mechanismus, der erlaubt die Gewichtung der Nodes eines Clusters anzugeben, nennt sich \textit{weighted Round-robin}.

\subsubsection{Test Szenarien} \label{ss:test}
Um eine geeignete Gewichtung der Nodes zu bestimmen müssen Test Szenarien entworfen werden, die eine ungleiche Lastverteilung in einem HiveMQ Cluster erzeugen.

\begin{itemize}
  \item \textbf{Szenario 1:} In \textit{Bare Metal} Deployments sind die Server Kapazitäten an die verfügbare Hardware verknüpft. Somit besteht die Möglichkeit, dass ein Cluster aus unterschiedlich dimensionierten Nodes besteht. Das HiveMQ Cluster besteht aus zwei Nodes mit 8 \ac{cpu} Kernen und 8 \ac{gb} \ac{ram} und einem Node mit 4 \ac{cpu} Kernen und 8 \ac{gb} \ac{ram}. Im Verlauf von zehn Minuten werden sich 5.000 Clients mit ähnlichem Verhalten mit dem Cluster verbinden und Nachrichten veröffentlichen sowie Topics abonnieren.
  \item \textbf{Szenario 2:} Eine dynamische Topologieänderung des HiveMQ Cluster kann durch mehrere Aktionen ausgelöst werden. Dazu zählen Rolling Upgrades oder Skalierung der Clusters aufgrund aufgebrauchter Ressourcen. In diesem Szenario wird ein zwei Node Cluster mit jeweils 8 \ac{cpu} Kernen und 8 \ac{gb} \ac{ram} pro Node mit einem neuen Node mit selben Dimensionen zur Laufzeit erweitert. Vor der Erweiterung sind 3.000 leichtgewichtige Clients mit dem Cluster verbunden. Nach der Erweiterung des Clusters verbinden sich 300 leistungsstarke Clients. Nach 5 Minuten verbinden sich weitere 300 leistungsstarke Clients.
  \item \textbf{Szenario 3:} Die Dimensionen des Clusters sind die selben wie in Szenario 2. In diesem Szenario verbinden sich die leistungsstarken Clients vor der Erweiterung und die leichtgewichtigen Clients nach der Erweiterung des Clusters.
\end{itemize}

Szenario 2 wird in mehrere Phasen untergliedert.
\begin{itemize}
  \item \textbf{Phase 1:} Formen des Clusters aus zwei Nodes (Node 1 und Node 2) mit jeweils 8 \ac{cpu} Kernen und 8 \ac{gb} \ac{ram}.
  \item \textbf{Phase 2:} Verbinden von 1.000 Subscribern, die zufällig 10 von 1000 Topics abonnieren wobei jedes Topic mindestens einmal abonniert wird.
  \item \textbf{Phase 3:} Verbinden von 3.000 Publishern. Jeder Publisher veröffentlicht zufällig alle 50 - 1.000 Millisekunden eine Nachricht mit der Payload \verb|Hello World|, einem zufälligen \ac{qos} Level auf ein zufälliges Topic.
  \item \textbf{Phase 4:} Ein dritter Node (Node 3) mit 8 \ac{cpu} Kernen und 8 \ac{gb} \ac{ram} tritt dem HiveMQ Cluster bei.
  \item \textbf{Phase 5:} Es verbinden sich 300 Publisher. Jeder Publisher veröffentlicht Nachrichten wie in Phase 3 beschrieben, jedoch zufällig alle 15 - 25 Millisekunden.
  \item \textbf{Phase 6:} Es verbinden sich 300 Publisher. Jeder Publisher veröffentlicht Nachrichten wie in Phase 4 beschrieben.
\end{itemize}
Szenario 3 hat die selben Phasen wie Szenario 2 in unterschiedlicher Reihenfolge. Phase 3 wird mit Phase 5 und 6 getauscht.

Alle Szenarien werden mit einem Round Robin und Least Connection Load Balancer Algorithmus ausgeführt um die Last der einzelnen Nodes zu untersuchen. Die \ac{cpu} Auslastung des Nodes wird als Lastindikator verwendet. Zur veranschaulichung der einzelnen Phasen wird neben der \ac{cpu} Auslastung auch die aktuelle Anzahl der Clients pro Node visualisiert.
Diese Metriken werden periodisch von allen Nodes des Clusters mit Prometheus erfasst und Grafana visualisiert.
% TODO wie ist die Funktion und wie sind die Settings
\\
\paragraph{Szenario 1}
% TODO map phasen mit bild
Round Robin und Least Connection Load Balancer verhalten sich in diesem Szenario ähnlich. Alle Clients werden gleichmä{\ss}ig auf die Nodes verteilt. Da die Clients ein ähnliches Verhalten aufweisen, ensteht auf jedem Node eine gleiche Belastung. Node 3 hat nur die hälfte der \ac{cpu} Kerne zur Verfügung wie Node 1 und 2 und ist daher nicht in der Lage die Last so schnell abzuarbeiten wie Node 1 oder 2. Somit ensteht ein ungleiches Lastverhalten im Cluster.

\paragraph{Szenario 2 - Least Connection}
% TODO map phasen mit bild
Nach Phase 5 ist die Last auf dem Cluster ungleich verteilt. Node 3 ist doppelt so sehr ausgelastet wie Node 1 und Node 2. Dieses Ungleichgewicht wird durch Phase 6 verstärkt. Auf Node 1 und Node 2 sind jeweils 2000 Clients verbunden auf Node 3 hingegen nur 300 Clients. Der Least Connection Load Balancer verteilt nun alle Clients aus Phase 6 auch auf Node 3. Trotz der nun noch höheren Auslastung auf Node 3 würden die nächsten 1600 Clients ebenfalls mit Node 3 verbunden werden.

\paragraph{Szenario 2 - Round Robin}
% TODO map phasen mit bild
Nach Phase 5 und 6 ist die Last auf dem Cluster im wesentlichen gleichmä{\ss}ig verteilt. Node 1 und 2 sind um den Betrag der Clients aus Phase 2 und 3 stärker belastet. Diese sind jedoch leichtgewichtige Clients und erzeugen demnach nicht viel Last auf den Nodes.
Der Round Robin Algorithmus verteilt alle Clients gleichmä{\ss}ig auf dem Cluster. Daher werden nach dem beitreten von Node 3 in Phase 4 nicht alle neuen Clients auf den neuen verteilt wie bei dem Least Connection Algorithmus.

\paragraph{Szenario 3 - Least Connection}
% TODO map phasen mit bild
Nach Phase 6 ist die Last im Cluster im wesentlichen gleichmä{\ss}ig verteilt. Die ersten 600 der 3000 Clients aus Phase 6 werden zwar mit Node 3 verbunden wodurch die ungleiche Lastverteilung nach dem beitreten des dritten Nodes in Phase 5 teilweise ausgeglichen wird. Die weiteren 2400 Clients werden danach auf alle drei Nodes verteilt. Da alle 3000 ein ähnliches Lastverhalten besitzen, wird diese Last auf dem Cluster gleichmä{\ss}ig verteilt.

\paragraph{Szenario 3 - Round Robin}
% TODO map phasen mit bild
Nach Phase 6 sind Node 1 und 2 jeweils doppelt so sehr ausgelastet wie Node 3. Die Last auf dem Cluster ist somit ungleich verteilt. Die Clients aus Phase 3 und 4 werden nur auf Node 1 und 2 verteilt, da der dritte Node noch nicht dem Cluster beigetreten ist. Die leichtgewichtigen Clients aus Phase 6 werden durch den Round Robing Algorithmus auf alle Nodes verteilt, obwohl Node 3 gar nicht ausgelastet ist. Im Idealfall würde der Load Balancer alle 3000 Clients aus Phase 6 auf Node 3 verteilen.
\\
TODO Fazit ??

\subsubsection{Weighted Round Robin} \label{ss:weighted-rr}
Kapitel \ref{ss:test} zeigt, dass ein Least Connection Balancer in Szenario 3 und ein Round Robin Load Balancer in Szenario 2 die Last gleichmä{\ss}ig verteilen kann. Bei den anderen Szenarien befindet sich der Cluster jedoch anschlie{\ss}end in einem ungleich Lastverhältnis. Da \ac{mqtt} auf langlebige \ac{tcp} Verbindungen basiert, wird sich dieses Lastverhältnis nicht verändern bis die Clients ihre Verbindung neu aufbauen.
Der Load Balancer muss dynamisch auf ein bestimmtes Szenario reagieren und Clients basierend der Arbeitslast der einzelnen Nodes verteilen.
Dabei werden die einzelnen Nodes basierend der aktuellen Arbeitslast gewichtet. Diese Gewichtung wird bei der Entscheidung, zu welchem Node ein neuer Client verbunden werden soll, mit eingezogen.
Diese Mechanik wird bereits bei anderen Protokollen wir \ac{http} eingesetzt. Bei \ac{mqtt} ist dies jedoch noch relevanter, da der Load Balancer die Last eines Clients nicht mehr steuern kann sobald dieser mit dem Cluster verbunden ist. Die Entscheidung, auf welchen Node der Client verbunden werden soll, ist demnach wichtig, denn diese kann für die Dauert der Verbindung nicht mehr verändert werden.
\\
Envoy hat mehrere Load Balancing Algorithmen eingebaut. Darunter befindet sich auch weighted Round-robin. Jedem Node eines Cluster wird eine Gewichtung in Form eines Integer Wertes zugeordnet. Der Wert muss grö{\ss}er als eins sein und die Werte alles Nodes addiert darf nicht grö{\ss}er als 4294967295 sein.
Um den Prozentsatz zu berechnen, wie viele Clients mit einem individuellen Node verbunden werden, teil Envoy die Gewichtung des jeweiligen Node durch die Summe der Gewichtungen aller Nodes und mulipliziert das Ergebnis mit 100.
\cite{SupportedLoadBalancers}
% TODO vielleicht die formel aufschreiben? sieht immer gut aus!
\\
Tabelle \ref{table:example-cluster-weight} zeigt drei Nodes mit ihren Gewichtungen und den dazu berechneten Prozentsatz aller Clients die mit diesem Node verbunden werden.

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|}
    \hline
    \textbf{Node} & \textbf{Gewichtung} & \textbf{Prozent Traffic} \\
    \hline
    \hline
    Node 1 & 10 & 40\% \\
    \hline
    Node 2 & 10 & 40\% \\
    \hline
    Node 3 & 5 & 20\% \\
    \hline
\end{tabular}
\caption{Nodes mit ihren Gewichtungen und berechnetem Prozensatz}
\label{table:example-cluster-weight}
\end{table}
% TODO better table caption
Der Quellcodeauszug \ref{code:envoy-cluster-weight} zeigt eine statische Envoy Konfiguration um ein Cluster aus drei Nodes zu formen mit den Gewichtungen wie in Tabelle \ref{table:example-cluster-weight} angezeigt.
\begin{figure}
    \import{gen/}{envoy-weighted-round-robin}
    \caption{Envoy weighted Round-robin Konfiguration}
    \label{code:envoy-cluster-weight}
\end{figure}
Die Problematik bei diesem Vorgehen ist, dass man die Nodes inklusiver Gewichtung statisch und manuell in der Konfigurationsdatei angeben muss.
Neben der statischen Konfigurationsdatei bietet Envoy zwei Mechanismen für eine dynamische Konfiguration an.
\begin{itemize}
  \item \textbf{Dynamisch via Dateisystem:} Envoy liest Konfigurationsdateien, die das \textit{xDS} Protokoll implementiert haben, vom Dateisystem ein. Wenn sich die Dateien auf dem Dateisystem ändern, aktualisiert Envoy automatisch seine Konfiguration.
    \cite{ConfigurationDynamicFilesystem}
  \item \textbf{Dynamisch via Control-Plane:} Envoy holt sich seine Konfiguration dynamisch von einer Control Plane ab. Eine Control Plane ist ein \ac{api} Server, der Envoy Konfigurationen an Envoy Server schickt. Um die Konfiguration an den Envoy zu schicken benutzt die Control Plane die \textit{Data-Plane API}\cite{EnvoyproxyDataplaneapi2021} von Envoy.
    \cite{ConfigurationDynamicControl}
\end{itemize}
Die dynamische Konfiguration via Dateisystem ist gut für einzelne Envoy Instanzen, da keine individuelle Control Plane programmiert werden muss. Betreibt man jedoch ein Envoy Cluster, in dem jede Envoy Instanz die selbe Konfiguration besitzt, muss man bei einer Aktualisierung der Konfiguration die neue Datei auf die Dateisysteme aller Envoys verteilen.
Für den Anwendungsfall eines Envoy Clusters ist das dynamische verteilen mit einer Control Plane besser geeignet. Jeder Envoy wird mit einer statischen Konfigurationsdatei konfiguriert, in der die Adresse der Control Plane eingetragen ist. Sobald ein Envoy startet registriert sich dieser an der Control Plane und erhält seine Konfiguration. Das Verteilen der Konfigurationsaktualisierungen an die Envoy Instanzen wird nun von der Control Plane übernommen.
\\
Quellcodeauszug \ref{code:envoy-control-plane} zeigt eine statische Konfigurationsdatei um sich bei einer Control Plane zu registrieren die unter \verb|example.control.plane:18000| erreichbar ist.
\begin{figure}
    \import{gen/}{envoy-control-plane}
    \caption{Statische Envoy Konfiguration um sich zu einer Control Plane zu verbinden}
    \label{code:envoy-control-plane}
\end{figure}
Die Konfigurationsdatei ist in drei Teile unterteilt:
\begin{itemize}
  \item \verb|node:| Identifikation der Envoy Instanz bei der Control Plane. Eine Control Plane ist in der Lage verschiedene Konfigurationen zu handhaben. Durch \verb|node.id| kann die Envoy Instanz einer Konfiguration zugeordnet werden.
  \item \verb|dynamic_resources:| Gibt die Quelle der dynamischen Provisionierung an. Im Beispiel \ref{code:envoy-control-plane} wird eine \verb|GRPC| \ac{api} Namens \verb|xds_cluster| verwendet.
  \item \verb|static_resources:| Es wird die Quelle für eine dynamische Provisionierung definiert. Im Beispiel \ref{code:envoy-control-plane} wird eine Quelle Namens \verb|xds_cluster| definiert. Diese Quelle wird in \verb|dynamic_resources| referenziert.
\end{itemize}
Eine Control Plane kann in jeder Programmiersprache entwickelt werden und muss nur die Spezifikationen der Data-Plane API \cite{EnvoyproxyDataplaneapi2021} berücksichtigen. Um den Einstieg für Entwickler zu erleichtern stellt Envoy Bibliotheken für Java und Golang bereit. Diese abstrahieren die Implementierung der Data-Plane API.
\\
Um die Skalierung der Envoy Instanzen zu ermöglichen wird im Rahmen der Thesis eine dynamische Konfiguration via Control Plane in Golang entwickelt. Der Grund für die Auswahl der Programmiersprache ist eine persönliche Präferenz. Wie erläutert, hat diese keine Auswirkung auf das Load Balancing des Envoys.

\subsubsection{Control Plane}
Envoy stellt eine Beispiels Control Plane \cite{DynamicConfigurationControl} bereit, die als Starthilfe für eine anwendungsspezifische Control Plane dient. Für den Anwedungsfall \ac{mqtt} soll die Control Plane eine Konfiguration ähnlich dem Beispiel \ref{code:envoy-weighted-round-robin} erzeugen. Dabei werden die Nodes und deren Gewichtung von der Control Plane bestimmt.
\\
Die in Kapitel \ref{ss:cluster-discovery} beschriebene HiveMQ Cluster Discovery kann zusammen mit einem gewichtenen Round-robin nicht implementiert werden. Wenn man den einzelnen Envoy Instanzen die Ermittlung der HiveMQ Nodes überlässt, hat man in der Control Plane keine Möglichkeit diesen HiveMQ Nodes individuelle Gewichtungen zu geben. Daher muss die Cluster Discovery in der Control Plane implementiert werden. Dem Envoy werden dann explizit alle HiveMQ Node \ac{ip} Adressen mit deren Gewichtungen und Port vermittelt.

\subsubsection{DNS Cluster Discovery}
Die Implementierung der HiveMQ Cluster Discovery in der Control Plane muss sich Konzeptuell der \ac{dns} Cluster Discovery von der HiveMQ Erweiterung anlehnen. Wie schon in Kapitel \ref{ss:cluster-discovery} beschrieben würde sonst die Topolgie des HiveMQ Clusters im Envoy mit der tatsächlichen Topologie divergieren.
Die Control Plane muss asynchron und periodisch einen gegebenen Domain Namen auflösen und alle hinterlegten \ac{ip} Adressen als Nodes eines HiveMQ Clusters eintragen. Das Interval der periodischen Auflösung sollte sehr gering gehalten werden, zum Beispiel fünf Sekunden, um auf Topologie Änderungen schnell zu reagieren. In der Regel sind in gro{\ss}en Infrastrukturen eigene \ac{dns} Server installiert, die eine hohe Abfragerate verarbeiten können.
Quellcodeauszug \ref{code:dns-resolve-net} zeige die Auflösung aller \ac{ip} Adressen eines Domain Namens in Golang mit der Standard \verb|net| Bibliothek. Da die Funktion \verb|refreshDNS| als Go Routine gestartet wird, kann die \verb|main| Funktion weiterlaufen und muss nicht auf die Terminierung der \verb|refreshDNS| Funktion warten. Die \verb|refreshDNS| Funktion erneuert die \ac{ip} Adressen periodisch in gegebenem Intervall.
\begin{figure}
    \import{gen/}{dns-resolve-net}
    \caption{\ac{ip} Adressen Auflösung eines Domain Namens in Golang}
    \label{code:dns-resolve-net}
\end{figure}

%\subsubsection{HiveMQ Metriken}
%% TODO vielleicht raus
%Eine Gewichtung der HiveMQ Nodes kann anhand der \ac{cpu} Auslastung der Nodes erstellt werden. Ein \ac{mqtt} Client ist jedoch in der Lage zur Laufzeit eine hohe Last auf dem Node zu erzeugen, indem er plötzlich viel mehr Nachrichten veröffentlich als zuvor. Dies sorgt für einen kurzfristigen Anstieg der \ac{cpu} Auslatung. Im Optimalfall merkt sich ein Broker diese Problemclients und bewertet die eigenen Auslastung höher als die \ac{cpu} Auslastung eigentlich angibt.
%HiveMQ stellt Metriken bereit und es gilt herauszuarbeiten, ob diese neben der \ac{cpu} Auslastung einen Indikator für die Auslastung des Nodes sind.
%Ein HiveMQ Broker stellt seine Metriken über folgende Schnittstellen bereit:
%\begin{itemize}
  %\item Prometheus
  %\item InfluxDB
  %\item Java Management Extension
%\end{itemize}
%\cite{MonitoringHiveMQDocumentation}
%Für Golang gibt es eine Bibliothek, die es ermöglicht Prometheus Metriken zu parsen. Grafana bietet zudem eine Anbindung für Prometheus um Metriken zu Visualisieren.\\
%Um das Monitoring von HiveMQ via Prometheus zu aktivieren, muss die \textit{Prometheus Monitoring Extension} \cite{HiveMQExtensionPrometheus} installiert werden. Um die Erweiterung zu konfigurieren kann eine Datei Namens \verb|/opt/hivemq/conf/prometheusConfiguration.properties| erstellt werden. Quellcodeauszug \ref{code:hivemq-prometheus-extension} zeigt eine Beispielskonfiguration.
%\begin{figure}
    %\import{gen/}{hivemq-prometheus-extension}
    %\caption{Beispielskonfiguration für die HiveMQ Prometheus Metriken Erweiterung}
    %\label{code:hivemq-prometheus-extension}
%\end{figure}
%Wie in Kapitel \ref{sb:overload-protection} erläutert, hat HiveMQ einen eingebauten Überlastschutz. Der aktuelle Status des Überlastschutzes kann auch über die Metriken abgefragt werden. HiveMQ stellt zudem auch noch Metriken des Host Systems, wie zum Beispiel die aktuelle \ac{cpu} Auslastung, und viele weitere Metriken bereit. Eine Liste aller Metriken findet sich in der HiveMQ Dokumentation \cite{MonitoringHiveMQDocumentation}. Tabelle \ref{table:overload-protection-metrics} gibt eine Übersicht der Metriken, die eine aktuelle Auslastung des Brokers widerspiegeln. Alle Metriken, die sich auf Clients beziehen, beinhalten nur die Daten der Clients, die auf diesem Node verbunden sind. Die Relevanz dieser Metriken für die Erstellung der Gewichtung eines Nodes wird in den Folgekapiteln untersucht.
%\cite{ClusterOverloadProtection}
%\begin{table}[htbp]
%\centering
%\renewcommand{\arraystretch}{1.5}
%\begin{tabularx}{\textwidth}{|p{5cm}|X|}
    %\hline
    %\textbf{Metrik} & \textbf{Beschreibung} \\
    %\hline
    %\hline
    %\verb|com.hivemq.supervision.| \verb|overload.protection.level| & Aktueller \textit{Overload Protection Level} zwischen 1 und 10 \\
    %\hline
    %\verb|com.hivemq.overload-| \verb|protection.credits.| \verb|per-tick| & Anzahl der Credits, die ein Client alle 200 Millisekunden erhält \\
    %\hline
    %\verb|com.hivemq.overload-| \verb|protection.clients.| \verb|average-credits| & Durchschnittliche Anzahl der Credits aller Clients \\
    %\hline
    %\verb|com.hivemq.overload-| \verb|protection.clients.| \verb|backpressure-active| & Anzahl der Clients, die durch eine Overload Protection \ac{tcp} Backpressure haben \\
    %\hline
    %\verb|com.hivemq.overload-| \verb|protection.global.tasks| & Anzahl der ausstehenden Aufgaben \\
    %\hline
    %\verb|com.hivemq.system.os.| \verb|global.cpu.total.total| & \ac{cpu} Auslastung \\
    %\hline
%\end{tabularx}
%\caption{HiveMQ Metriken, die eine Auskunft über die Auslastung des Brokers geben}
%\label{table:overload-protection-metrics}
%\end{table}
% TODO update the cpu metric to match the correct one

\subsubsection{Weighted CPU Round Robin} \label{ss:weighted-cpu}
Die Testszenarien aus Kapitel \ref{ss:test} zeigen, dass es bei \ac{mqtt} viele verschieden Client Verhaltensmuster gibt. Ein Client kann sein Verhalten zur Laufzeit um 180 Grad verändern und der Load Balancer ist nicht mehr in der Lage diesen Client mit einem anderen Node zu verbinden. Der Node muss das neue Verhalten des Clients nun verarbeiten.
Daher korreliert eine \ac{tcp} Verbindung bei \ac{mqtt} nicht mit der verursachten Arbeitslast auf dem Broker. In Szenario 2 haben 300 Publisher doppelt so viel Last erzeugt wie 3000 Subscriber und 1000 Publisher zusammen.
Die Anzahl der Verbindungen eines Nodes reicht im Fall \ac{mqtt} nicht aus um eine geeignete Load Balancing Entscheidung zu treffen.

Über eine Control Plane kann man die Gewichtung einzelner Nodes eines Cluster im Envoy dynamisch anpassen.
Die Control Plane muss periodisch und asynchron die aktuelle \ac{cpu} Auslastung eines jeden Nodes abfragen und darauf basierend die Nodes gewichten.
HiveMQ stellt die \ac{cpu} Auslastung über die Prometheus Erweiterung unter dem Schlüssel \verb|com_hivemq_system_os_global_cpu_total_total| bereit.
Die Prometheus Metriken kann man über einen in der Konfiguration angegebenen Endpunkt jederzeit über \ac{http} Abfragen. Standardmä{\ss}ig befindet sich der Endpunkt unter \verb|http://localhost:9399/metrics|.
Quellcodeauszug \ref{code:prometheus-curl} zeigt die Abfrage der Metriken eines HiveMQ Brokers gefiltert nach \verb|cpu_total_total|. Der Datentyp dieser Metrik ist ein \textit{Gauge}, der einen einzelnen numerischen Wert darstellt.\cite{prometheusMetricTypesPrometheus} Der Wert beträgt in diesem Beispiel \verb|5.0| und bedeutet, dass die aktuelle \ac{cpu} Auslastung fünf Prozent beträgt.
\begin{figure}
    \import{gen/}{prometheus-curl}
    \caption{Abfrage von Prometheus Metriken mit curl und grep in einem Terminal.}
    \label{code:prometheus-curl}
\end{figure}
Für die programmatische Auswertung von Prometheus Metriken stellt Prometheus mehrere Bibliotheken für verschiedene Programmiersprachen bereit. Darunter befindet sich ebenfalls eine Bibliothek für Golang. Das \verb|expfmt| Paket dieser Bibliothek ist für das Parsen der Metriken zuständig.\cite{ExpfmtPkgGo}
Quellcodeauszug \ref{code:expfmt} zeigt ein Golang Programm, das periodisch und asynchron die Prometheus Metrik \verb|com_hivemq_system_os_global_cpu_total_total| von einem HiveMQ Broker unter der Adresse \verb|http://localhost:9399/metrics| abruft. Der Wert der Metrik kann alle Werte zwischen 0 (keine Auslastung) und 100 (volle Auslastung) annehmen. Er wird in der Variable \verb|value| als Gleitkommazahl gespeichert.
\begin{figure}
    \import{gen/}{expfmt}
    \caption{Abfragen und parsen von Prometheus Metriken in Golang mit \textit{expfmt} und \textit{net/http}.}
    \label{code:expfmt}
\end{figure}
\\

% TODO Abbildung
Abbildung \ref{figure:} zeigt, dass der \ac{cpu} Wert eines Nodes innerhalb von Minuten um fünf bis zehn Prozen schwanken kann. Mögliche Ursachen sind zum Beispiel der Java Garbage Collector, eine kurzfristige Traffic Spitze oder ein Client-Takeover.
Diese kurzfristigen Unterschiede der Auslastung dürfen nicht direkt zu einer Veränderung der Gewichtung des Nodes führen. Es ist möglich, dass die Auslastung eines Nodes in den letzten 10 Minuten im Durchschnitt geringer ist, als bei allen anderen im Cluster, aber zu dem Zeitpunkt, wenn ein neuer Client eine Verbindung mit dem Cluster aufbaut, die Auslastung des Nodes am höchsten ist, weil in diesem Moment ein Client mehrere Nachrichten veröffentlich hat, der dies aber nur halbstündlich tut. Wenn die Control Plane diese Auslastungsspitze sofort in die Gewichtung mit einberechnet, würde der neue Client nicht auf den Node mit der durchschnittlich geringsten Auslastung verbunden werden.
Um den direkten Einfluss solcher Auslastungsspitzen zu vermeiden, werden Werte über einen bestimmten Zeitraum gesammelt und gemittelt. Der Mittelwert ist somit die durchschnittliche Arbeitslast eines Nodes in Prozent für Dauer \verb|x|.
\\
Als Zeitraum der zu mittelnden Werte wird die Dauer von einer Minute gewählt. Wenn die Abtastfrequenz der \ac{cpu} Werte fünf Sekunden beträgt, müssen die letzten \verb|60 / 5 = 12| Werte aufsummiert und durch die Anzahl der Werte, 12, geteilt werden.

% TODO code snippet that measures median of x values

Als finalen Schritt muss nun die durchschnittliche Arbeitslast in eine Gewichtung für Envoy konvertiert werden. Je grö{\ss}er der Wert der Gewichtung im Vergleich zu den Gewichtungen der anderen Nodes ist, je mehr Clients werden mit diesem Node verbunden. In Kapitel \ref{ss:weighted-rr} wurde die Berechnung bereits beschrieben. Demnach ist es nicht möglich die durchschnittliche Arbeitslast direkt als Gewichtung zu benutzen. Es würde ein gegenteiliger Effekt erzielt werden.
Angenommen es gibt vier Nodes mit einer durchschnittlichen Auslastung nach Tabelle \ref{table:example-cluster-cpu}. Die freie Auslastung ergibt sich aus der Differenz von maximaler Auslastung (100) und durchschnittlicher Auslastung. Wenn man die die freie Auslastung als Gewichtung der Nodes wählt, verteilen sich neueh Clients wie folgt auf die Nodes:
\begin{itemize}
  \item \textbf{Node 1:}
    \begin{align}
      13 / (13 + 70 + 95 + 78) \cdot 100 \approx 5,1 \%
    \end{align}
  \item \textbf{Node 2:}
    \begin{align}
      70 / (13 + 70 + 95 + 78) \cdot 100 \approx 27,3 \%
    \end{align}
  \item \textbf{Node 3:}
    \begin{align}
      95 / (13 + 70 + 95 + 78) \cdot 100 \approx 37,1 \%
    \end{align}
  \item \textbf{Node 4:}
    \begin{align}
      78 / (13 + 70 + 95 + 78) \cdot 100 \approx 30,5 \%
    \end{align}
\end{itemize}
Nodes mit einer höheren Auslastung, zum Beispiel Node 1, erhalten nun weniger neue Clients als Nodes mit einer geringeren Auslastung wie Node 3.

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|}
    \hline
    & \textbf{Ø CPU Auslastung} & \textbf{Ø freie CPU Kapazitäten} \\
    \hline
    \hline
    Node 1 & 87 \% & 13 \% \\
    \hline
    Node 2 & 30 \% & 70 \% \\
    \hline
    Node 3 & 5 \% & 95 \% \\
    \hline
    Node 4 & 22 \% & 78 \% \\
    \hline
\end{tabular}
\caption{CPU Auslastung und Kapazitäten von Nodes eines Clusters über einen beliebigen Zeitraum}
\label{table:example-cluster-cpu}
\end{table}

\subsubsection{Circuit Breaking} \label{ss:circuit-breaking}
% TODO explain circuit breaking?
Wie in Kapitel \ref{sb:overload-protection} beschrieben, hat ein HiveMQ Broker mehrere Möglichkeiten sich vor einer Überlastsituation zu schützen. Dabei kann ein Broker im ersten Schritt \ac{tcp} Backpressure auf individuelle Client Verbindungen anwenden um den Paketfluss zu verzögern. Falls dies nicht ausreicht um eine Lastspitze einzudämmen, fängt der Broker an Verbindungen von Clients zu beenden, die ihre Credits aufgebraucht haben. Dadurch werden gezielt Clients vom Broker getrennt, die zu viel Arbeitslast für den Broker verursachen. Durch die Trennung dieser Clients gibt es keine Service Eischränkungen für alle anderen Clients die mit dem Broker verbunden sind. Wenn der Broker Clients, die zu viel Arbeitslast verursachen, nicht abwerfen würde, ist er zu sehr damit beschäftigt die Anfragen dieser Clients zu bearbeiten und die Sericequalität für alle anderen Clients wird eingeschränkt.
\\
Wenn sich ein Node eines Clusters in einer Überlastsituation befindet, ist es wichtig, dass der Load Balancer keine neuen Clients mehr mit diesem Node verbindet.
Der aktuelle Status der Overload Protection kann über die Prometheus Metriken abgefragt werden. Eine Liste aller Prometheus Metriken findet sich in der HiveMQ Dokumentation \cite{MonitoringHiveMQDocumentation}. Tabelle \ref{table:overload-protection-metrics} gibt eine Übersicht der Metriken, die eine aktuelle Überlastung des Brokers widerspiegeln. Metriken, die sich auf Clients beziehen, beinhalten nur die Daten der Clients, die auf dem Node verbunden sind, bei dem die Metriken abgefragt werden.\cite{ClusterOverloadProtection}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabularx}{\textwidth}{|p{5cm}|X|}
    \hline
    \textbf{Metrik} & \textbf{Beschreibung} \\
    \hline
    \hline
    \verb|com.hivemq.supervision.| \verb|overload.protection.level| & Aktueller \textit{Overload Protection Level} zwischen 1 und 10 \\
    \hline
    \verb|com.hivemq.overload-| \verb|protection.credits.| \verb|per-tick| & Anzahl der Credits, die ein Client alle 200 Millisekunden erhält \\
    \hline
    \verb|com.hivemq.overload-| \verb|protection.clients.| \verb|average-credits| & Durchschnittliche Anzahl der Credits aller Clients \\
    \hline
    \verb|com.hivemq.overload-| \verb|protection.clients.| \verb|backpressure-active| & Anzahl der Clients, die durch eine Overload Protection \ac{tcp} Backpressure haben \\
    \hline
\end{tabularx}
\caption{HiveMQ Metriken, die eine Auskunft über die Overload Protection des Brokers geben}
\label{table:overload-protection-metrics}
\end{table}
Die Metrik \verb|com.hivemq.supervision.overload.protection.level| aggregiert intern mehrere Metriken zusammen und bestimmt einen allgemeinen Überlast Level des Nodes. Der Wert des Levels liegt zwischen 0 und 10 wobei 10 das höhchste Überlastlevel ist.
Damit ein Überlastlevel von 10 möglichst nicht erreicht wird, kann der Load Balancer präventiv keine neuen Clients mehr mit einem Node verbinden, der einen bestimmten Schwellenwert als Level überschritten hat.
\\
Ein hohes Überlastlevel kann kurzfristig durch Events wie das Beitreten eines neuen Nodes in ein Cluster auftreten. Dabei werden einige Client Queues auf den neuen Node repliziert, was zeitweise die Nodes, auf denen die Queues liegen, stark beansprucht. Daher ist es wichtig, dass der Load Balancer auf solche Events schnell reagiert und keine neuen Clients mehr mit den betroffenen Nodes verbindet.
Um dies zu gewährleisten, wird eine geringe Abtastfrequenz von 2 Sekunden der Prometheus Metriken gewählt.
% TODO tests zeigen wie so ein wert aussehen kann
Eine in Abbildung \ref{figure:} gezeigte Überlastsituation verursacht einen maximalen Overload Protection Level von drei. Die Überlastsituation wurde nur von Client Verbindungen erzeugt. Durch \ac{tcp} Backpressure kann der Broker eine solche Überlastsituation gut verarbeiten ohne die Verbindung der Clients von dem Broker zu unterbrechen. Bei TODO ZEITSTEMPEL ist ein neuer Node in der Cluster beigetreten und hat einen Overload Protection Level von 7 erzeugt. Um in einer solcher Situation keine neuen Clientverbindungen zuzulassen, kann der Schwellenwert für den Überlastschutz auf 5 gesetzt werden.
\\
Nodes eines Clusters in Enovy haben einen \textit{Health Status} der angibt, ob diese neue Clients akzeptieren können oder nicht. Der Wert ist entweder \verb|HEALTHY| oder \verb|UNHEALTHY|. Dieser Zustand kann neben der Load Balancing Gewichtung über die Control Plane für jeden Node individuell gesetzt werden.
%https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/core/v3/health_check.proto#envoy-v3-api-enum-config-core-v3-healthstatus
\\
In der Control Plane werden nun asynchron und periodisch alle zwei Sekunden die Prometheus Metriken wie in Kapitel \ref{ss:weighted-cpu} beschrieben von allen Nodes des Clusters abgerufen. Sobald die Metrik \verb|com.hivemq.supervision.overload.protection.level| einen Wert von über fünf erreicht, wird der Health Status des Nodes auf \verb|UNHEALTHY| gesetzt. Der Load Balancer verbindet nun keine neuen Clients mehr mit diesem Node. Sobald der Wert wieder unter fünf sinkt, wird der Status auf \verb|HEALTHY| gesetzt und es können wieder neue Clients mit dem Node verbunden werden.

\subsubsection{Health Check}
In Kapitel \ref{ss:circuit-breaking} wurde beschrieben, dass man über die Control Plane den Health Status der Nodes eines Clusters angeben kann.


\subsubsection{Aktualisierung der Konfiguration}
Um Aktualisierungen an eine Envoy Instanz zu schicken werden versionierte Momentaufnahmen (engl. \textit{Snapshots}) von einem Zustand der Konfiguration erstellt. Wenn die Version des Snapshots auf dem Envoy eine andere Version ist als die Version des aktuellen Snapshots auf der Control Plane, liefert die Control Plane den Snapshot an die Envoy Instanz aus.
Für die Entwicklung einer Control Plane bedeutet dies, sobald sich die Konfiguration ändert muss, sich auch die Version ändern, damit die neue Konfiguration an die Envoys ausgeliefert wird. Wenn die Konfiguration einen Zustand annimmt, für den sie in der Vergangenheit schon eine Version gebildet hat, sollte die Control Plane diese Version wiederverwenden. Somit kann man Snapshots mit ihrere Version in einem Cache-Speicher hinterlegen.
\\
Um bei der gleichen Cluster Konfiguration immer die selbe Version zu benutzen kann man die Cluster Konfiguration in eine Hash Funktion geben und bekommt einen String mit fixer Länge als Rückgabewert zurück. Die Hash Funktion garantiert einen immer gleichen Rückgabewert bei gleichem Input.
% TODO hash beschreiben
Die Eingabe der Hash Funktion sind alle Informationen bei deren Änderung eine neue Version erstellt werden soll. Dies ist eine Liste aller HiveMQ Nodes mit folgenden Werten:
\begin{itemize}
  \item \ac{ip} Adresse
  \item Port
  \item Gewichtung
  \item Status
\end{itemize}
Die Reihenfolge, in der die Nodes in der Liste vorkommen, ist von relevanz. Falls alle Nodes die selben Werte haben, sich aber die Reihenfolge der Liste geändert hat, würde die Hash Funktion einen neuen Wert zurückgeben. Daher muss sichergestellt werden, dass die Nodes immer in der gleichen Reihenfolge vorkommen. Um die Liste der Nodes zu sortieren muss überlegt werden welche Werte des Nodes als Vergleich benutzt werden. Dafür müssen Eigenschaften definiert werden, die einen Node einzigartig machen.
Es können niemals zwei HiveMQ Nodes die selbe \ac{ip} Adresse und den selben Port haben. Beide Werte als String vereint können somit miteinander verglichen werden um bei gleichbleibenden Nodes die selbe Reihenfolge in einer Liste einzuhalten.
% TODO soll man dafür ein beispiel machen?

\subsection{Sticky Session}
\subsubsection{Client Identifier}
\subsubsection{MQTT CONNECT}

\subsubsection{Hash?}
% clientid hash bestimmt das backend
%\subsubsection{Envoy WASM Network Filter}

\begin{comment}
- This is supposed to be the core of your thesis or project. Describe your work from a con-ceptual viewpoint.
- Example: In case you have developed some prototypical tool in your bachelor thesis, demonstrate how it is employed in its business context. More concrete example: Assume that your contribution is a Maven-Build-Plugin that further automates the deployment of changes to the claim handling process into production. In this case show how the plugin is integrated in the overall (continuous) integration and deployment process, which human ac-tors are involved, which external systems and so on. Elaborate on subtle edge cases you had to deal with, e.g., possible outages of external systems.
- Usedi  agramswhere appropriate. Standard notations are better than informal box-and-line-diagrams. Typical standard notations for a solution concept are
  - Business Process Modelling Notation (BPMN) or UML activity diagrams, that depict a workflow in which your tool is used
  - UML component diagrams, where your tool is represented by just a single component (without its ingredients) together with connected external systems
\end{comment}
