\section{Lösungskonzept} \label{s:solution}
Im folgenden Kapitel werden alle Problematiken aus Kapitel \ref{s:problem} detailliert analysiert und konzeptuelle Lösungsansätze erarbeitet.
In Kapitel \ref{ss:cluster-discovery} wird die Integration einer dynamischen HiveMQ Cluster Discover in Envoy beschrieben.
Kapitel \ref{ss:load-distribution} untersucht die Lastverteilung in einem HiveMQ Cluster mit Fokus auf die gleichmä{\ss}ige Verteilung von \ac{mqtt} Clients anhand einer Envoy Control Plane (siehe Kapitel \ref{ss:control-plane}).
Abschlie{\ss}end wird in Kapitel \ref{ss:sticky-session} die Problemstellung der persistenten Client Sessions (siehe Kapitel \ref{sp:persistent-session}) behandelt.

\subsection{Cluster Discovery} \label{ss:cluster-discovery}
HiveMQ hat mehrere Mechanismen um die individuellen Nodes zu entdecken, die in das Cluster aufgenommen werden sollen (siehe \ref{s:hivemq-cluster}).
Ein \acl{lb} muss in der Lage sein, das HiveMQ Cluster auf derselben Datenbasis wie HiveMQ zu formen.
Wenn der \ac{lb} eine andere Datenbasis benutzen würde, dann könnte die Topologie des HiveMQ Clusters im \ac{lb} von der tatsächlichen Topologie abweichen.\\
Envoy verfügt über folgende Möglichkeiten Nodes eines Clusters zu bestimmen:
\begin{itemize}
  \item \textbf{Static:} Alle Nodes eines Clusters werden statisch in die Envoy Konfiguration eingetragen.
  \item \textbf{Strict \ac{dns}:} Envoy löst periodisch und asynchron einen konfigurierten \ac{dns} Namen auf. Jede eingetragene \ac{ip} Adresse wird zu einem Node des Clusters. Falls ein Node entfernt wird, werden keine neuen Clients mehr mit diesem Node verbunden. Mit der Variable \verb|dns_refresh_rate| kann die Frequenz, in welcher der \ac{dns} Eintrag abgefragt wird, bestimmt werden.
  \item \textbf{Logical \ac{dns}:} Ähnlich wie bei Strict \ac{dns} löst Envoy einen konfigurierten \ac{dns} Namen auf. Bei jeder neuen eingehenden Verbindung wird der \ac{dns} Name erneut aufgelöst und die erste \ac{ip} Adresse als Ziel der neuen Verbindung genommen.
  \item \textbf{Original Destination:} Envoy leitet eingehende Verbindungen anhand der \textit{Redirect Metadata} weiter. Eingehende Verbindungen müssen dafür mit einem \textit{iptables REDIRECT}, \textit{TPROXY target} oder \textit{Proxy Protocol} an Envoy weitergeleitet werden.
  \item \textbf{Endpoint Discovery Service:} Envoy ruft die Nodes eines Cluster bei einem \textit{xDS Management Server} ab. Es werden Java und Golang Bibliotheken angeboten um einen Management Server für Envoy zu programmieren und bereitzustellen. Somit ist es möglich eine komplexe Service Discovery zu implementieren.
\end{itemize}
\cite{ServiceDiscoveryEnvoy}
In Kapitel \ref{s:hivemq-cluster} wurden folgende Methoden der HiveMQ Cluster Discovery erläutert:
\begin{itemize}
  \item static
  \item multicast
  \item broadcast
  \item extension
\end{itemize}
Envoy und HiveMQ stellen eine statische Cluster Discovery zur Verfügung. Diese Methode ist für Cloud- oder Containerumgebungen nicht optimal, da sie keine Möglichkeit bietet die Cluster Topologie dynamisch zu verändern. Container Management Umgebungen, wie zum Beispiel Kubernetes, erlauben dynamische Vorgänge wie das Skalieren der Replika-Sets oder Rolling-Updates. Eine statische Clusterkonfiguration schlie{\ss}t den Einsatz dieser oder ähnlichen dynamischen Vorgängen aus.\\
Eine weitere gemeinsame Cluster Discovery Methode ist Strict \ac{dns}. HiveMQ hat diese Methode nicht in der Standardversion eingebaut, stellt für diesen Anwendungsfall aber eine frei zugängliche Erweiterung bereit.
Bei Strict \ac{dns} werden periodisch alle Einträge zu einem gegebenen \ac{dns} Eintrag abgefragt und als Nodes des Cluster anerkannt. Die Frequenz der Abfrage kann in Envoy mit der Variable \verb|dns_refresh_rate| bestimmt werden. In der HiveMQ Erweiterung ist die Einstellung der Frequenz derzeit noch nicht möglich. Ein Issue \cite{AllowConfigurationDiscovery} und ein Pull Request \cite{ExponentialBackoffGeneral} wurden bereits zu diesem Feature auf GitHub erstellt.
\\
Anders als bei der statischen Clusterkonfiguration, ist bei der Strict \ac{dns} Methode eine dynamische Änderung der Nodes möglich. In Cloudumgebungen wie Kubernetes werden \ac{dns} Zonen dynamisch von Plugins wie \textit{CoreDNS} verwaltet.\cite{DNSServicesPods}
Die Strict \ac{dns} Methode kann auf Änderungen dieser Zonen reagieren und dynamisch die Topologie des HiveMQ Clusters anpassen.
\\
Der Quellcodeauszug \ref{code:envoy-strict-dns} zeigt eine Envoy Clusterkonfiguration, die den \ac{dns} Namen\newline \verb|example.cluster.internal| auflöst und neue Verbindungen auf alle Einträge an den Port \verb|1883| verteilt.
\begin{figure}
    \import{gen/}{envoy-strict-dns}
    \caption{Envoy Strict \ac{dns} Konfiguration}
    \label{code:envoy-strict-dns}
\end{figure}
Die \textit{HiveMQ DNS Cluster Discovery Extension} \cite{HiveMQExtensionDNS} muss auf allen Nodes installiert sein. Der Quellcodeauszug \ref{code:hivemq-dnsdiscovery} zeigt eine Konfigurationsdatei der Erweiterung, die das Cluster aus allen Einträgen des \ac{dns} Namens \verb|example.cluster.internal| bildet.
\begin{figure}
    \import{gen/}{dnsdiscovery}
    \caption{HiveMQ \ac{dns} Cluster Discovery Konfiguration}
    \label{code:hivemq-dnsdiscovery}
\end{figure}

\subsection{Lastverteilung HiveMQ Cluster} \label{ss:load-distribution}
In Kapitel \ref{sp:load} wurde erläutert, dass in einem HiveMQ Cluster eine ungleiche Lastverteilung durch die verschiedenen Verhaltensweisen der Clients auftreten kann.
In dem Fall, dass ein Node eine höhere Belastung als ein anderer Node aufweist, möchte man die Verteilung neuer Clients an die entstandene Situation anpassen. Es sollen mehr Clients an Nodes mit geringerer Belastung verbunden werden.
Durch den weighted round-robin load balancing Algorithmus kann die Verteilung neuer Clients angepasst werden. Individuelle Gewichtungen pro Node bestimmen die Wahrscheinlichkeit, mit der ein neuer Client mit einem Node verbunden wird.

\subsubsection{Testszenarien} \label{ss:test}
Um Algorithmen zur Bestimmung von Gewichtungen der Nodes bei einem weighted round-robin Algorithmus zu evaluieren, werden Testszenarien entworfen, die eine ungleiche Lastverteilung in einem HiveMQ Cluster erzeugen.

\begin{itemize}
  \item \textbf{Szenario 1:} In \textit{Bare Metal} Deployments sind die Server Kapazitäten an die verfügbare Hardware geknüpft. Somit besteht die Möglichkeit, dass ein Cluster aus unterschiedlich dimensionierten Nodes besteht. In Szenario eins besteht das HiveMQ Cluster aus zwei Nodes mit acht \ac{cpu} Kernen und acht \ac{gb} \ac{ram} und einem Node mit vier \ac{cpu} Kernen und acht \ac{gb} \ac{ram}. Im Verlauf von zehn Minuten werden sich 5.000 Clients mit ähnlichem Verhalten mit dem Cluster verbinden und Nachrichten veröffentlichen sowie Topics abonnieren.
  \item \textbf{Szenario 2:} Eine dynamische Topologieänderung des HiveMQ Cluster kann durch mehrere Aktionen ausgelöst werden. Dazu zählen Rolling Upgrades oder Skalierungen des Clusters aufgrund Überdimensionierung oder aufgebrauchter Ressourcen. In diesem Szenario wird ein zwei Node Cluster mit jeweils acht \ac{cpu} Kernen und acht \ac{gb} \ac{ram} pro Node mit einem neuen Node mit selben Dimensionen zur Laufzeit erweitert. Vor der Erweiterung sind 3.000 leichtgewichtige Clients mit dem Cluster verbunden. Nach der Erweiterung des Clusters verbinden sich 600 leistungsstarke Clients.
  \item \textbf{Szenario 3:} Die Dimensionen des Clusters sind dieselben wie in Szenario zwei. In diesem Szenario verbinden sich die leistungsstarken Clients vor der Erweiterung und die leichtgewichtigen Clients nach der Erweiterung des Clusters.
\end{itemize}

Szenario zwei wird in folgende Phasen untergliedert:
\begin{itemize}
  \item \textbf{Phase 1:} Formen des Clusters aus zwei Nodes (Node eins und zwei) mit jeweils acht \ac{cpu} Kernen und acht \ac{gb} \ac{ram}.
  \item \textbf{Phase 2:} Verbinden von 3.000 Subscribern, die zufällig 10 von 1000 Topics abonnieren wobei jedes Topic mindestens einmal abonniert wird.
  \item \textbf{Phase 3:} Verbinden von 2.000 Publishern. Jeder Publisher veröffentlicht zufällig alle 500 - 2.000 Millisekunden eine Nachricht mit der Payload \verb|Hello World|, einem zufälligen \ac{qos} Level auf ein zufälliges Topic.
  \item \textbf{Phase 4:} Ein dritter Node (Node drei) mit acht \ac{cpu} Kernen und acht \ac{gb} \ac{ram} tritt dem HiveMQ Cluster bei.
  \item \textbf{Phase 5:} Es verbinden sich 300 Publisher. Jeder Publisher veröffentlicht Nachrichten wie in Phase drei beschrieben, jedoch zufällig alle 15 - 25 Millisekunden.
  \item \textbf{Phase 6:} Es verbinden sich 300 Publisher. Jeder Publisher veröffentlicht Nachrichten wie in Phase vier beschrieben.
\end{itemize}
Szenario drei hat dieselben Phasen wie Szenario zwei in unterschiedlicher Reihenfolge. Phase drei wird mit Phase fünf und sechs getauscht.
\\
Alle Szenarien werden mit einem round-robin und least connection load balancing Algorithmus ausgeführt, um die Last der einzelnen Nodes zu untersuchen. Die \ac{cpu} Auslastung des Nodes wird als Lastindikator verwendet. Zur Veranschaulichung der einzelnen Phasen wird neben der \ac{cpu} Auslastung auch die aktuelle Anzahl der Clients pro Node visualisiert.
Alle Metriken werden periodisch mit Prometheus erfasst und mit Grafana visualisiert.
\\
Um einen Indikator für die Verteilung der Auslastung im Cluster zu erstellen, werden die Beträge der Differenz der \ac{cpu} Auslastung aller Nodes zueinander addiert. Je grö{\ss}er das Ergebnis, desto ungleichmä{\ss}iger ist die Last verteilt.
Bei einem Cluster aus drei Nodes mit den \ac{cpu} Auslastungen 50, 20 und 30 beträgt der berechnete Indikator:
\begin{align}
    |Node 1 - Node 2| + |Node 1 - Node 3| + |Node 2 - Node 3| & = Lastindikator
    \\
    |50 - 20| + |50 - 30| + |20 - 30| & = 60
\end{align}
Im Vergleich hat ein Cluster mit den \ac{cpu} Auslastungen 23, 30 und 18 folgenden Indikator:
\begin{align}
   |23 - 30| + |23 - 18| + |30 - 18| = 24
\end{align}
Durch diesen Indikator, folgend Lastindikator genannt, kann die Lastverteilung von Clustern der selben Topolgie verglichen werden.

\paragraph{Szenario 1}
Abbildung \ref{fig:s1-rr} und \ref{fig:s1-lc} zeigen den zeitlichen Verlauf von Testszenario eins für einen round-robin und least connection \acl{lb}.
Der round-robin und least connection \ac{lb} verhalten sich in diesem Szenario ähnlich. Alle Clients werden gleichmä{\ss}ig auf die Nodes verteilt.
Nachdem alle Clients mit dem Cluster verbunden sind, entsteht bei dem round-robin \ac{lb} ein Lastindikator von 40 und bei dem least connection \ac{lb} ein Lastindikator von 42.
Da die Clients ein ähnliches Verhalten aufweisen, entsteht auf jedem Node eine gleiche Belastung. Node drei hat nur die Hälfte der \ac{cpu} Kerne zur Verfügung wie Node eins und zwei und ist daher nicht in der Lage die Last so schnell abzuarbeiten wie Node eins oder zwei. Somit entsteht ein ungleiches Lastverhalten im Cluster.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s1_lc.png}
    \caption{Testszenario 1 - Least Connection Load Balancer}
    \label{fig:s1-lc}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s1_rr.png}
    \caption{Testszenario 1 - Round Robin Load Balancer}
    \label{fig:s1-rr}
\end{figure}

\paragraph{Szenario 2 - Least Connection}
Abbildung \ref{fig:s2-lc} zeigt den zeitlichen Verlauf von Testszenario zwei bei einem least connection \ac{lb}.
Nach Abschluss von Phase sechs bei 14:45 hat dieses Szenario einen Lastindikator von 50.
Die Last auf dem Cluster ungleich verteilt da Node drei doppelt so sehr ausgelastet wie Node eins und Node zwei. Dieses Ungleichgewicht wird durch Phase sechs verstärkt. Auf Node eins und Node zwei sind jeweils 3500 Clients verbunden auf Node drei hingegen nur 300 Clients. Der Least Connection Load Balancer verteilt nun alle Clients aus Phase sechs auch auf Node drei. Trotz der nun noch höheren Auslastung auf Node drei würden die nächsten 2900 Clients ebenfalls mit Node drei verbunden werden.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s2_lc.png}
    \caption{Testszenario 2 - Least Connection Load Balancer}
    \label{fig:s2-lc}
\end{figure}

\paragraph{Szenario 2 - Round Robin}
Abbildung \ref{fig:s2-rr} zeigt den zeitlichen Verlauf von Testszenario zwei bei einem round-robin \ac{lb}.
Nach Abschluss von Phase sechs bei 10:20 hat dieses Szenario einen Lastindikator von 16.
Nach Phase fünf und sechs ist die Last auf dem Cluster im wesentlichen gleichmä{\ss}ig verteilt. Node eins und zwei sind um den Betrag der Clients aus Phase zwei und drei stärker belastet. Diese sind jedoch leichtgewichtige Clients und erzeugen demnach nicht viel Last auf den Nodes.
Der round-robin Algorithmus verteilt alle Clients gleichmä{\ss}ig auf dem Cluster. Daher werden nach dem Beitreten von Node drei in Phase vier nicht alle neuen Clients auf den neuen Node verteilt wie bei dem least connection Algorithmus.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s2_rr.png}
    \caption{Testszenario 2 - Round Robin Load Balancer}
    \label{fig:s2-rr}
\end{figure}

\paragraph{Szenario 3 - Least Connection}
Abbildung \ref{fig:s3-lc} zeigt den zeitlichen Verlauf von Testszenario drei bei einem least connection \ac{lb}.
Nach Abschluss von Phase sechs bei 11:18 hat dieses Szenario einen Lastindikator von 24.
Node eins und zwei sind mehr belastet als Node drei, da alle Clients aus Phase drei und vier mit diesen beiden Nodes verbunden sind. Alle Clients aus Phase sechs sind mit Node drei verbunden, jedoch verursachen diese nicht so viel Arbeitslast wie die Clients aus Phase drei und vier.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s3_lc.png}
    \caption{Testszenario 3 - Least Connection Load Balancer}
    \label{fig:s3-lc}
\end{figure}

\paragraph{Szenario 3 - Round Robin}
Abbildung \ref{fig:s3-rr} zeigt den zeitlichen Verlauf von Testszenario drei bei einem round-robin \ac{lb}.
Nach Abschluss von Phase sechs bei 10:57 hat dieses Szenario einen Lastindikator von 40.
Nach Phase sechs sind Node eins und zwei jeweils dreifach so sehr ausgelastet wie Node drei. Die Last auf dem Cluster ist somit ungleich verteilt.
Die Clients aus Phase drei und vier werden nur auf Node eins und zwei verteilt, da der dritte Node noch nicht dem Cluster beigetreten ist. Die leichtgewichtigen Clients aus Phase sechs werden durch den round-robin Algorithmus auf alle Nodes verteilt, obwohl Node drei gar nicht ausgelastet ist. Im Idealfall würde der \ac{lb} alle 3000 Clients aus Phase sechs auf Node drei verteilen.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s3_rr.png}
    \caption{Testszenario 3 - Round Robin Load Balancer}
    \label{fig:s3-rr}
\end{figure}

\subsubsection{Weighted Round Robin} \label{ss:weighted-rr}
Kapitel \ref{ss:test} zeigt, dass ein least connection \acl{lb} in Szenario drei und ein round-robin \ac{lb} in Szenario zwei, die Last gleichmä{\ss}ig verteilen kann. Bei den anderen Szenarien befindet sich das Cluster anschlie{\ss}end in einem ungleichen Lastverhältnis. Da \ac{mqtt} auf langlebigen \ac{tcp} Verbindungen basiert, wird sich dieses Lastverhältnis nicht verändern, bis die Clients ihre Verbindung neu aufbauen oder ihr Verhalten verändern.
Der \ac{lb} muss dynamisch auf ein bestimmtes Client-Verhalten reagieren und Clients basierend der Arbeitslast der einzelnen Nodes verteilen.
\\
Bei dem weighted round-robin load balancing Algorithmus wird jedem Node eines Clusters eine Gewichtung zugeordnet. Die Gewichtung des Nodes bestimmt die Wahrscheinlichkeit, mit der ein neuer Client mit einem Node verbunden wird.
Um mehr Clients an Nodes mit geringerer Auslastung zu verbinden, muss die derzeitige Auslastung direkten Einfluss auf die Gewichtung des Nodes haben.
Diese Mechanik wird bereits bei anderen Protokollen wie \ac{http} eingesetzt.
Eine load balancing Entscheidung bei \ac{mqtt} ist jedoch relevanter als bei Protokollen wie \ac{http}, weil die Entscheidung für die Dauer der Verbindung nicht mehr verändert werden kann. Da \ac{mqtt} von langlebigen \ac{tcp} Verbindungen profitiert, kann der \ac{lb} die Last eines bestimmten Clients nach dem Verbindungsaufbau nicht mehr steuern.
\\
Envoy hat mehrere load balancing Algorithmen, unter anderem weighted round-robin, implementiert. Jedem Node eines Cluster wird eine Gewichtung in Form eines Integer Wertes zugeordnet. Je grö{\ss}er der Wert im Verhältnis zu allen anderen Nodes des Cluster ist, desto höher ist die Wahrscheinlichkeit mit der ein neuer Client mit dem Node verbunden wird. Der Wert der Gewichtung muss grö{\ss}er als eins sein und die Werte aller Nodes addiert dürfen nicht grö{\ss}er als 4294967295 sein.
Um den Prozentsatz zu berechnen, wie viele Clients mit einem individuellen Node verbunden werden, teilt Envoy die Gewichtung des jeweiligen Nodes durch die Summe der Gewichtungen aller Nodes und multipliziert das Ergebnis mit 100.
\cite{SupportedLoadBalancers}
\\
Tabelle \ref{table:example-cluster-weight} zeigt drei Nodes mit ihren Gewichtungen und den dazu berechneten Prozentsatz aller Clients die mit diesem Node verbunden werden.
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|}
    \hline
    \textbf{Node} & \textbf{Gewichtung} & \textbf{Prozent Traffic} \\
    \hline
    \hline
    Node 1 & 10 & 40\% \\
    \hline
    Node 2 & 10 & 40\% \\
    \hline
    Node 3 & 5 & 20\% \\
    \hline
\end{tabular}
\caption{Nodes mit ihren Gewichtungen und berechnetem Prozensatz aller Clients}
\label{table:example-cluster-weight}
\end{table}
\\
Der Quellcodeauszug \ref{code:envoy-cluster-weight} zeigt eine statische Envoy Konfiguration, um ein Cluster aus drei Nodes zu formen mit den Gewichtungen aus Tabelle \ref{table:example-cluster-weight}.
\begin{figure}
    \import{gen/}{envoy-weighted-round-robin}
    \caption{Envoy weighted Round-robin Konfiguration}
    \label{code:envoy-cluster-weight}
\end{figure}
\\
Die Problematik bei der statischen Konfiguration aus Quellcodeauszug \ref{code:envoy-cluster-weight} ist, dass man die Nodes inklusiver Gewichtung statisch und manuell in der Konfigurationsdatei angeben muss. Eine dynamische Cluster Discovery wie in Kapitel \ref{ss:cluster-discovery} beschrieben ist nicht mehr möglich.
Neben der statischen Konfigurationsdatei bietet Envoy zwei Mechanismen für eine dynamische Konfiguration an.
\begin{itemize}
  \item \textbf{Dynamisch via Dateisystem:} Envoy liest Konfigurationsdateien, die das xDS Protokoll implementiert haben, vom Dateisystem ein. Wenn sich die Dateien auf dem Dateisystem ändern, aktualisiert Envoy automatisch seine Konfiguration.
    \cite{ConfigurationDynamicFilesystem}
  \item \textbf{Dynamisch via Control Plane:} Envoy holt sich seine Konfiguration dynamisch von einer Control Plane ab. Eine Control Plane ist ein \ac{api} Server, der Konfigurationen an Envoy Instanzen schickt. Die Schnittstelle um Konfigurationen zwischen Envoy und Control Plane auszutauschen ist in der \textit{Data-Plane API} (siehe \cite{EnvoyproxyDataplaneapi2021}) dokumentiert.
    \cite{ConfigurationDynamicControl}
\end{itemize}
Die dynamische Konfiguration via Dateisystem ist gut für einzelne Envoy Instanzen, da keine individuelle Control Plane programmiert werden muss.
Möchte man jedoch mehrere Envoy Instanzen mit derselben Konfiguration betreiben, muss bei einer Aktualisierung der Konfiguration die neue Datei auf die Dateisysteme aller Envoys verteilt werden.
Für den Anwendungsfall mehrerer Envoy Instanzen ist das dynamische Verteilen mit einer Control Plane besser geeignet. Jeder Envoy wird mit einer statischen Konfigurationsdatei konfiguriert, in der die Adresse der Control Plane eingetragen ist. Sobald eine Envoy Instanz startet, registriert sich diese an der Control Plane und erhält seine Konfiguration. Das Verteilen von Konfigurationsaktualisierungen an die Envoy Instanzen wird nun von der Control Plane übernommen.
\\
Quellcodeauszug \ref{code:envoy-control-plane} zeigt eine statische Konfigurationsdatei, um sich bei einer Control Plane zu registrieren die unter \verb|example.controlplane.internal:18000| erreichbar ist.
\begin{figure}
    \import{gen/}{envoy-control-plane}
    \caption{Statische Envoy Konfiguration um sich mit einer Control Plane zu verbinden}
    \label{code:envoy-control-plane}
\end{figure}
Die Konfigurationsdatei ist in drei Teile unterteilt:
\begin{itemize}
  \item \verb|node:| Identifikation der Envoy Instanz bei der Control Plane. Eine Control Plane ist in der Lage verschiedene Konfigurationen zu verwalten. Durch \verb|node.id| kann die Envoy Instanz einer Konfiguration zugeordnet werden.
  \item \verb|dynamic_resources:| Gibt die Quelle der dynamischen Provisionierung an. Im Quellcodeauszug \ref{code:envoy-control-plane} wird eine \verb|gRPC| \ac{api} Namens \verb|xds_cluster| verwendet.
  \item \verb|static_resources:| Es wird die Quelle für eine dynamische Provisionierung definiert. Im Quellcodeauszug \ref{code:envoy-control-plane} wird eine Quelle Namens \verb|xds_cluster| definiert. Diese Quelle wird in \verb|dynamic_resources| referenziert.
\end{itemize}
Eine Control Plane kann in jeder Programmiersprache entwickelt werden und muss nur die Spezifikationen der Data-Plane API \cite{EnvoyproxyDataplaneapi2021} berücksichtigen. Um den Einstieg für Entwickler zu erleichtern, stellt Envoy Bibliotheken für Java und Golang bereit, die eine Abstraktion der Data-Plane \ac{api} bieten.
\\
Um eine horizontale Skalierung der Envoy Instanzen zu ermöglichen, wird im Rahmen der Thesis eine dynamische Konfiguration via Control Plane in Golang entwickelt.

\subsubsection{Control Plane} \label{ss:control-plane}
Eine Envoy Control Plane wird als eigenständiger Webservice betrieben und muss von allen Envoy Instanzen, die ihre Konfiguration von der Control Plane erhalten sollen, erreichbar sein.
\\
Abbildung \ref{fig:control-plane-architecture} zeigt den Fluss des Traffics bei einem Envoy als \acl{lb}, konfiguriert von einer Control Plane, und einem HiveMQ Cluster. Die Envoy Instanz erhält ihre Listener- und Clusterkonfiguration von der Control Plane. \ac{mqtt} Clients verbinden sich mit dem \ac{lb} und werden an einen entsprechenden Node des HiveMQ Clusters weitergeleitet.
\\
Envoy stellt eine beispielhafte Control Plane \cite{DynamicConfigurationControl} bereit, die als Starthilfe für eine anwendungsspezifische Control Plane dient. Für den Anwendungsfall \ac{mqtt} soll die Control Plane eine Konfiguration ähnlich dem Quellcodeauszug \ref{code:envoy-cluster-weight} erzeugen. Dabei werden die Nodes und deren Gewichtung von der Control Plane bestimmt.
\\
Die in Kapitel \ref{ss:cluster-discovery} beschriebene HiveMQ Cluster Discovery schlie{\ss}t die Benutzung des weighted round-robin Algorithmus in Envoy aus.
Bei der Strict \ac{dns} Cluster Discovery Methode ermitteln die Envoy Instanzen die Nodes eines Clusters und propagieren diese nicht an die Control Plane zurück. Somit kann die Control Plane den einzelnen Nodes keine spezifischen Gewichtungen vergeben.
Anstatt den Envoy Instanzen einen Domain Namen mitzuteilen und diesen auflösen zu lassen, muss der \ac{dns} Cluster Discovery Mechanismus in der Control Plane implementiert werden. Den Envoy Instanzen werden somit explizit alle HiveMQ Node \ac{ip} Adressen mit Gewichtung und Port mitgeteilt.
\begin{figure}
    \centering
    \includegraphics[scale=0.36]{images/control-plane-architecture.png}
    \caption{Architektur eines Envoy Load Balancers konfiguriert von einer Control Plane mit einem HiveMQ Cluster und MQTT Clients}
    \label{fig:control-plane-architecture}
\end{figure}

\subsubsection{DNS Cluster Discovery} \label{ss:dns-discovery}
Die Implementierung der HiveMQ Cluster Discovery in der Control Plane muss sich Konzeptuell der \ac{dns} Cluster Discovery von der HiveMQ Erweiterung anlehnen. Wie schon in Kapitel \ref{ss:cluster-discovery} beschrieben, würde sonst die Topolgie des HiveMQ Clusters im Envoy von der tatsächlichen Topologie divergieren.
\\
Die Control Plane muss asynchron und periodisch einen gegebenen Domain Namen auflösen und alle hinterlegten \ac{ip} Adressen als Nodes eines HiveMQ Clusters eintragen. Das Intervall der periodischen Auflösung sollte sehr gering gehalten werden, um auf Topologieänderungen schnell reagieren zu können.
Cloudumgebungen wie Kubernetes haben in der Regel eigene \ac{dns} Services installiert, die eine hohe Abfragerate verarbeiten können.
\\
Quellcodeauszug \ref{code:dns-resolve-net} zeige die Auflösung aller \ac{ip} Adressen eines Domain Namens in Golang mit der Standard \verb|net| Bibliothek. Da die Funktion \verb|refreshDNS| als Go Routine gestartet wird, muss die \verb|main| Funktion nicht auf die Terminierung der \verb|refreshDNS| Funktion warten. Die \verb|refreshDNS| Funktion erneuert die \ac{ip} Adressen periodisch in gegebenem Intervall.
\begin{figure}
    \import{gen/}{dns-resolve-net}
    \caption{\ac{ip} Adressen Auflösung eines Domain Namens in Golang}
    \label{code:dns-resolve-net}
\end{figure}

\subsubsection{Weighted CPU Round Robin} \label{ss:weighted-cpu}
Die Testszenarien aus Kapitel \ref{ss:test} zeigen, dass es bei \ac{mqtt} viele verschiedene Client Verhaltensmuster gibt. Zudem kann ein Client sein Verhalten zur Laufzeit verändern und der \acl{lb} ist nicht mehr in der Lage diesen Client mit einem anderen Node zu verbinden.
Eine \ac{tcp} Verbindung bei \ac{mqtt} korreliert nicht mit der verursachten Arbeitslast auf dem Broker. In Szenario zwei haben 300 Publisher doppelt so viel Last erzeugt wie 3000 Subscriber und 1000 Publisher zusammen.
Nur anhand der Anzahl aktiver Verbindungen pro Node kann im Fall \ac{mqtt} keine optimale load balancing Entscheidung getroffen werden.
\\
In Kapitel \ref{ss:test} wurde bereits die \ac{cpu} Auslastung als Indikator für die Arbeitslast eines Nodes gewählt. Die \ac{cpu} Auslastung ergibt sich aus der Anzahl der Anweisungen, die eine \ac{cpu} in bestimmter Zeit ausführt.
HiveMQ stellt die aktuelle \ac{cpu} Auslastung über die Prometheus Metriken zur Verfügung. Der \acl{lb} muss die Gewichtung der einzelnen Nodes anhand dieser Metriken bestimmen, damit Nodes mit geringerer Auslastung mehr Clients erhalten. Somit kann verbindungsunabhängig eine gleichmä{\ss}ige Verteilung der Last im Cluster erfolgen.
\\
Über eine Control Plane kann die Gewichtung einzelner Nodes eines Cluster im Envoy dynamisch angepasst werden.
Die Control Plane muss periodisch und asynchron die aktuelle \ac{cpu} Auslastung eines jeden Nodes abfragen und darauf basierend die Nodes gewichten.
Quellcodeauszug \ref{code:prometheus-curl} zeigt die Abfrage der Prometheus Metriken eines HiveMQ Brokers gefiltert nach \verb|cpu_total_total|. Der Datentyp dieser Metrik ist ein \textit{Gauge}, der einen einzelnen numerischen Wert darstellt.\cite{prometheusMetricTypesPrometheus} Der Wert beträgt in diesem Beispiel 5.0 und bedeutet, dass die aktuelle \ac{cpu} Auslastung fünf Prozent beträgt.
\begin{figure}
    \import{gen/}{prometheus-curl}
    \caption{Abfrage von Prometheus Metriken mit curl und grep in einem Terminal.}
    \label{code:prometheus-curl}
\end{figure}
\\
Für die programmatische Auswertung von Prometheus Metriken werden Bibliotheken für verschiedene Programmiersprachen bereitgestellt. Darunter befindet sich ebenfalls eine Bibliothek für Golang. Das \verb|expfmt| Paket dieser Bibliothek ist für das Parsen der Metriken zuständig.\cite{ExpfmtPkgGo}
Quellcodeauszug \ref{code:expfmt} zeigt ein Golang Programm, das periodisch und asynchron die Prometheus Metrik \verb|com_hivemq_system_os_global_cpu_total_total| von einem HiveMQ Broker unter der Adresse \verb|http://localhost:9399/metrics| abruft. Der Wert der Metrik kann alle Werte zwischen 0 (keine Auslastung) und 100 (volle Auslastung) annehmen. Er wird in der Variable \verb|value| als Gleitkommazahl gespeichert.
\begin{figure}
    \import{gen/}{expfmt}
    \caption{Abfragen und parsen von Prometheus Metriken in Golang mit \textit{expfmt} und \textit{net/http}.}
    \label{code:expfmt}
\end{figure}
\\
Die \ac{cpu} Auslastung eines Nodes kann durch diverse Faktoren zeitweise schwanken. Mögliche Ursachen sind zum Beispiel der Java Garbage Collector, eine kurzfristige Traffic Spitze oder ein Client-Takeover.
Diese kurzfristigen Unterschiede der Auslastung dürfen nicht direkt zu einer Veränderung der Gewichtung des Nodes führen.
Um den direkten Einfluss solcher Spitzen zu vermeiden, werden Werte über einen bestimmten Zeitraum gesammelt und gemittelt. Der Mittelwert ist somit die durchschnittliche Arbeitslast eines Nodes in Prozent für eine bestimmte Dauer.
\\
Anschlie{\ss}end muss die durchschnittliche Arbeitslast in eine Gewichtung für Envoy konvertiert werden. Je grö{\ss}er der Wert der Gewichtung im Vergleich zu den Gewichtungen der anderen Nodes ist, je mehr Clients werden mit diesem Node verbunden (siehe Kapitel \ref{ss:weighted-rr}). Demnach ist es nicht möglich die durchschnittliche Arbeitslast direkt als Gewichtung zu benutzen. Es würde ein gegenteiliger Effekt entstehen.
\\
Da Nodes mit grö{\ss}eren Gewichtungen mehr Clients erhalten, wird die freie \ac{cpu} Auslastung als Gewichtung der Nodes gewählt.
Die freie Auslastung ergibt sich aus der Differenz von maximaler Auslastung (100) und durchschnittlicher Auslastung.
Wenn man die freie Auslastung als Gewichtung der Nodes wählt, verteilen sich neue Clients bei einem Cluster aus Tabelle \ref{table:example-cluster-cpu} wie folgt auf die Nodes:
\begin{itemize}
  \item \textbf{Node 1:}
    \begin{align}
      13 / (13 + 70 + 95 + 78) \cdot 100 \approx 5,1 \%
    \end{align}
  \item \textbf{Node 2:}
    \begin{align}
      70 / (13 + 70 + 95 + 78) \cdot 100 \approx 27,3 \%
    \end{align}
  \item \textbf{Node 3:}
    \begin{align}
      95 / (13 + 70 + 95 + 78) \cdot 100 \approx 37,1 \%
    \end{align}
  \item \textbf{Node 4:}
    \begin{align}
      78 / (13 + 70 + 95 + 78) \cdot 100 \approx 30,5 \%
    \end{align}
\end{itemize}
Nodes mit einer höheren Auslastung, zum Beispiel Node 1, erhalten nun weniger neue Clients als Nodes mit einer geringeren Auslastung wie Node 3.
\\
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|}
    \hline
    & \textbf{Ø CPU Auslastung} & \textbf{Ø freie CPU Kapazitäten} \\
    \hline
    \hline
    Node 1 & 87 \% & 13 \% \\
    \hline
    Node 2 & 30 \% & 70 \% \\
    \hline
    Node 3 & 5 \% & 95 \% \\
    \hline
    Node 4 & 22 \% & 78 \% \\
    \hline
\end{tabular}
\caption{CPU Auslastung und Kapazitäten von Nodes eines Clusters über einen beliebigen Zeitraum}
\label{table:example-cluster-cpu}
\end{table}
Um einen weighted \ac{cpu} round-robin \acl{lb} zu validieren, wurden die Testszenarien aus Kapitel \ref{ss:test} ebenfalls durchgeführt. Tabelle \ref{table:test-output} zeigt die Lastindikatoren der einzelnen Testszenarien verglichen mit einem round-robin und least connection \ac{lb}. Abbildungen \ref{fig:s1-cpu}, \ref{fig:s2-cpu} und \ref{fig:s3-cpu} zeigen die zeitlichen Verläufe des weighted \ac{cpu} Load Balancers bei den Testszenarien 1, 2 und 3.
Insgesamt hat der weighted round-robin \ac{lb} besser abgeschnitten als der least connection \ac{lb} und genauso gut wie der round-robin \ac{lb}.
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s1_cpu.png}
    \caption{Testszenario 1 - Weighted CPU Load Balancer}
    \label{fig:s1-cpu}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s2_cpu.png}
    \caption{Testszenario 2 - Weighted CPU Load Balancer}
    \label{fig:s2-cpu}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/s3_cpu.png}
    \caption{Testszenario 3 - Weighted CPU Load Balancer}
    \label{fig:s3-cpu}
\end{figure}

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|}
    \hline
    & Round Robin & Least Connection & Weighted CPU \\
    \hline
    Testszenario 1 & 40 & 42 & 40 \\
    \hline
    Testszenario 2 & 16 & 50 & 12 \\
    \hline
    Testszenario 3 & 40 & 24 & 44 \\
    \hline
    \hline
    Gesamt & 96 & 116 & 96 \\
    \hline
\end{tabular}
\caption{Lastindikatorentabelle der Testszenarien aus Kapitel \ref{ss:test}}
\label{table:test-output}
\end{table}
% TODO warum sind die abweichungen zwischen den algorithmen so gro{\ss} ?

\subsubsection{Überlastschutz} \label{ss:circuit-breaking}
Wie in Kapitel \ref{ss:weighted-cpu} beschrieben, kann das Lastverhalten von \ac{mqtt} Clients während der Laufzeit stark variieren. Da ein Client zur Laufzeit nicht mit einem anderen Node verbunden werden kann, kann es bei dem Verändern des Lastverhaltens von Clients zu einer Überlastsituation eines Nodes kommen.
Ein HiveMQ Broker hat mehrere Möglichkeiten sich vor einer solchen Überlastsituation zu schützen (siehe Kapitel \ref{sb:overload-protection}).
Im ersten Schritt wendet der Broker \ac{tcp} Backpressure auf individuelle Clientverbindungen an, um den Paketfluss zu verzögern.
Falls durch diese Mechanik eine Lastspitze nicht unter Kontrolle gebracht werden kann, wird die Verbindung von Clients unterbrochen, die ihre Credits aufgebraucht haben.
Dadurch werden gezielt Clients vom Broker getrennt, die zu viel Arbeitslast für den Broker verursachen.
Durch die Trennung dieser Clients gibt es keine Serviceeinschränkungen für alle anderen Clients, die mit dem Broker verbunden sind. Andernfalls kann es zu Paketverzögerungen oder Paketverlusten auf einem Broker kommen.
\\
Wenn sich ein Node eines Clusters in einer Überlastsituation befindet, ist es wichtig, dass der Load Balancer keine neuen Clients mehr mit diesem Node verbindet.
Über die Prometheus Schnittstelle der HiveMQ Nodes können diverse Metriken über den Zustand eines Nodes abgefragt werden.
Eine Liste aller Prometheus Metriken findet sich in der HiveMQ Dokumentation \cite{MonitoringHiveMQDocumentation}.
Tabelle \ref{table:overload-protection-metrics} gibt eine Übersicht der Metriken, die Indikatoren für eine mögliche Überlastsituation sind. Metriken, die sich auf Clients beziehen, enthalten nur Daten von Clients, die mit dem Node verbunden sind, bei dem die Metriken abgefragt wurden.
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabularx}{\textwidth}{|p{5cm}|X|}
    \hline
    \textbf{Metrik} & \textbf{Beschreibung} \\
    \hline
    \hline
    \verb|com.hivemq.supervision.| \verb|overload.protection.level| & Aktueller \textit{Overload Protection Level} zwischen 1 und 10 \\
    \hline
    \verb|com.hivemq.overload-| \verb|protection.credits.| \verb|per-tick| & Anzahl der Credits, die ein Client alle 200 Millisekunden erhält \\
    \hline
    \verb|com.hivemq.overload-| \verb|protection.clients.| \verb|average-credits| & Durchschnittliche Anzahl der Credits aller Clients \\
    \hline
    \verb|com.hivemq.overload-| \verb|protection.clients.| \verb|backpressure-active| & Anzahl der Clients, die durch eine Overload Protection \ac{tcp} Backpressure haben \\
    \hline
\end{tabularx}
\caption{HiveMQ Metriken, die eine Auskunft über die Overload Protection des Brokers geben}
\label{table:overload-protection-metrics}
\end{table}
\\
Die Metrik \verb|com.hivemq.supervision.overload.protection.level| aggregiert intern mehrere Metriken zusammen und bestimmt einen allgemeinen Überlastlevel des Nodes. Der Wert des Levels liegt zwischen null und zehn, wobei zehn das höchste Überlastlevel ist.
Damit das maximale Überlastlevel möglichst nicht erreicht wird, kann der \acl{lb} präventiv keine neuen Clients mehr mit einem Node verbinden, der einen bestimmten Schwellwert als Level überschritten hat.
\\
Ein hohes Überlastlevel kann kurzfristig durch Events wie das Beitreten eines neuen Nodes in ein Cluster auftreten. Dabei werden einige Client Queues auf den neuen Node repliziert, was zeitweise die Nodes, auf denen die Queues liegen, stark beansprucht. Daher ist es wichtig, dass der \ac{lb} auf solche Events schnell reagiert und keine neuen Clients mehr mit den betroffenen Nodes verbindet.
Um dies zu gewährleisten, sollte eine geringe Abtastfrequenz der Prometheus Metriken gewählt werden.
\\
Eine in Abbildung \ref{fig:overload-protection} gezeigte Überlastsituation verursacht einen maximalen Überlastschutzlevel von 1,5.
Die Überlastsituation wurde nur von Clientverbindungen erzeugt. Durch \ac{tcp} Backpressure kann der Broker eine solche Überlastsituation gut verarbeiten, ohne die Verbindung der Clients von dem Broker zu unterbrechen. Wenn ein neuer Node in das Cluster aufgenommen wird, kann das Überlastlevel auf bis zu zehn ansteigen. Um nur in einer extremen Situation keine neuen Clientverbindungen zuzulassen, kann der Schwellenwert für den Überlastschutz auf vier gesetzt werden.
\\
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/overload-protection.png}
    \caption{HiveMQ Cluster das Client TCP Backpressure anwendet}
    \label{fig:overload-protection}
\end{figure}
Um Envoy zu instruieren keine neuen Clients mit einem sich in Überlastsituation befindenden HiveMQ Node zu verbinden, wird der \textit{Health Status} des Nodes entsprechend gesetzt.
Der Wert ist entweder \verb|HEALTHY| oder \verb|UNHEALTHY|. Dieser Zustand kann über die Control Plane für jeden Node individuell gesetzt werden.
\cite{HealthCheckEnvoy}
Die Funktionalität des \textit{Health Status} eines Nodes wird in Kapitel \ref{ss:health-check} detailliert betrachtet.
\begin{comment}
\\
In der Control Plane werden nun asynchron und periodisch alle zwei Sekunden die Prometheus Metriken wie in Kapitel \ref{ss:weighted-cpu} beschrieben von allen Nodes des Clusters abgerufen. Sobald die Metrik \verb|com.hivemq.supervision.overload.protection.level| einen Wert von über fünf erreicht, wird der Health Status des Nodes auf \verb|UNHEALTHY| gesetzt. Der Load Balancer verbindet nun keine neuen Clients mehr mit diesem Node. Sobald der Wert wieder unter fünf sinkt, wird der Status auf \verb|HEALTHY| gesetzt und es können wieder neue Clients mit dem Node verbunden werden.
\end{comment}

\subsubsection{Health Check} \label{ss:health-check}
Nodes eines Clusters in Envoy haben einen Gesundheitszustand (engl. \textit{Health Status}) der angibt, ob ein Node neue Clientverbindungen erhalten kann.
Der Health Status eines Nodes ist anwendungsspezifisch und kann unter anderem durch \textit{health checks} bestimmt werden.
\ac{http} Services implementieren für diesen Zweck beispielsweise eine \verb|/health| Route, die der \acl{lb} periodisch abrufen kann. Wenn der \ac{http} Status Code der Antwort 200 beträgt, ist der Service healthy und bereit für neue Verbindungen.
\\
Um zu überprüfen, ob ein HiveMQ Broker bereit ist neue Clientverbindungen \ac{mqtt} konform anzunehmen, muss sich die Control Plane als \ac{mqtt} Client bei dem Broker anmelden. Falls die Verbindung erfolgreich ist, sollte die \ac{mqtt} Verbindung \ac{mqtt} konform beendet und der Node als \verb|HEALTHY| markiert werden.
Um einen ausführlicheren Health Check zu implementieren, können noch weitere \ac{mqtt} Funktionen wie Publish und Subscribe getestet werden. Dazu abonniert die Control Plane ein Topic, das nicht von anderen Clients verwendet wird. Nach dem Abonnieren veröffentlicht die Control Plane eine beliebige Nachricht auf dem zuvor abonnierten Topic und überprüft, ob diese Nachricht empfangen wird. Abbildung \ref{fig:health-check-sequence} zeigt die erforderlichen \ac{mqtt} Pakete und den Ablauf des beschriebenen Health Checks in einem UML Ablaufdiagramm. Wenn keine Fehler während des Health Checks auftreten, wird der Node als \verb|HEALTHY| markiert.
\\
Um Clients möglichst immer mit einem funktionierenden HiveMQ Node eines Clusters zu verbinden, muss der Health Check periodisch von der Control Plane ausgeführt werden.
Zudem kann ein Zeitfenster definiert werden, in dem ein Health Check erfolgreich durchgelaufen sein muss. Falls der Health Check länger als das Zeitfenster braucht, gilt der Node als \verb|UNHEALTHY|. Dadurch kann neben der Funktionalität des Brokers auch eine gegebene Antwortzeit gewährleistet werden.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{gen/health-check.png}
    \caption{UML Ablaufdiagramm eines MQTT Health Checks}
    \label{fig:health-check-sequence}
\end{figure}

\begin{comment}
Sobald die \ac{dns} Cluster Discovery aus Kapitel \ref{ss:dns-discovery} einen neuen Node entdeckt hat, muss überprüft werden, ob der Node bereits dem Cluster beigetreten ist. Bei der \ac{dns} Cluster Discovery besteht die Möglichkeit, dass die Control Plane einen anderen \ac{dns} Server benutzt als die HiveMQ Nodes. Wenn zum Zeitpunkt des Startes des neuen Nodes die \ac{ip} Adresse des Nodes noch nicht in der \ac{dns} Registry aller Nodes eingetragen ist, dann startet dieser Node als eigenständiger Broker und ist nicht Teil des Clusters.
Alle Clients die mit diesem Broker verbunden werden, können nicht mit den Clients aus dem Cluster kommunizieren. Dies führt zu einem Datenleck, da zum Beispiel Nachrichten, die auf dem Cluster auf dem Topic \verb|sensors| veröffentlicht werden, nicht an Clients weitergeleitet werden, die auf dem einzelnen Broker das Topic \verb|sensors| abonniert haben.
Der Load Balancer darf erst Clients mit einem neuen Node verbinden, wenn dieser Teil des Clusters ist.
\\
Die Prometheus Metrik des HiveMQ Brokers \verb|com.hivemq.cluster.nodes.count| gibt an, wie viele Nodes Teil des HiveMQ Clusters sind. Mit Hilfe dieser Metrik lässt sich also feststellen, ob die Grö{\ss}e des Cluster mit der Anzahl der entdeckten Nodes übereinstimmt. Angenommen es gibt ein HiveMQ Cluster bestehend aus drei Nodes. Nun entdeckt die \ac{dns} Cluster Discovery einen vierten Node und fragt bei allen vier Nodes die Metrik \verb|com.hivemq.cluster.nodes.count| ab. Wenn der Wert der Metrik bei den bisherigen Nodes \verb|3| ist und bei dem neu entdeckten Node \verb|1|, dann ist der neue Node nicht Teil des Clusters und der Load Balancer darf keine Clients mit dem Node verbinden. Wenn die Werte aller Nodes \verb|4| sind, kann der Load Balancer davon ausgehen, das der Node Teil des Clusters ist.
% TODO golang code ? oder vielleicht auch nicht! einfach in der implementierung zeigen
\\
Ein Nachteil dieser Methode ist, dass jeder Node auch Teil eines anderen Clusters sein könnte. Angenommen es gibt vier HiveMQ Cluster bestehend aus je vier Nodes. Ein beliebiger Node eines jeden Clusters wird in die Domain eingetragen. Die Metrik \verb|com.hivemq.cluster.nodes.count| würde nun bei allen Nodes den Wert \verb|4| liefern. Für die Control Plane ist nun jeder Node Teil des selben Clusters, obwohl dies nicht der Fall ist. HiveMQ bietet keine Möglichkeit abzufragen, ob Nodes Teil des selben Clusters sind.
\\
Wenn die \ac{dns} Cluster Discovery einen neuen Node entdeckt hat, der noch nicht Teil des Cluster ist, ist davon auszugehen, dass dieser Node noch in der Cluster aufgenommen wird. Der Prozess, bei dem ein neuer Node in ein bestehendes HiveMQ Cluster aufgenommen wird, hei{\ss}t \textit{Cluster Join}. Je nachdem wie viele Client Queues auf dem Cluster angelegt sind, kann der Cluster Join entweder einige Sekunden bis hin zu fünfzehn Minuten dauern. In dieser Zeit muss der Load Balancer diesen Node als \verb|UNHEALTHY| markieren und erst auf \verb|HEALTHY| setzen, wenn der Wert der Cluster Node Metrik bei allen Nodes übereinstimmt und der Anzahl der Nodes entspricht.
\end{comment}

\subsubsection{Aktualisierung der Konfiguration}
Um Aktualisierungen wie Health Status, Gewichtung oder Anzahl der Nodes an eine Envoy Instanz zu schicken, werden versionierte Momentaufnahmen (engl. \textit{Snapshots}) von einem Zustand der Konfiguration erstellt. Wenn die Version des Snapshots auf dem Envoy eine andere Version ist als die Version des aktuellen Snapshots auf der Control Plane, liefert die Control Plane den aktuellen Snapshot an die Envoy Instanz aus.
\\
Um Konfigurationen optimal in einen Cache-Speicher ablegen zu können, sollte eine einzigartige Konfiguration immer dieselbe Version haben.
Dazu kann die Clusterkonfiguration in eine Hash Funktion gegeben werden, um als Resultat einen String mit fixer Länge als Rückgabewert zu kriegen. Die Hash Funktion garantiert einen immer gleichen Rückgabewert bei gleichem Input. Dementsprechend wird bei gleichbleibender Konfiguration immer die gleiche Version erzeugt.
Als Eingabe der Hash Funktion dienen alle Informationen bei deren Änderung eine neue Version erstellt werden soll. Dies ist eine Liste aller HiveMQ Nodes mit folgenden Werten:
\begin{itemize}
  \item \ac{ip} Adresse
  \item Port
  \item Gewichtung
  \item Health Status
\end{itemize}
Die Reihenfolge, in der die Nodes in der Liste für die Hash Funktion vorkommen, ist von Relevanz. Falls alle Nodes dieselben Werte haben, sich aber die Reihenfolge der Liste geändert hat, würde die Hash Funktion einen neuen Wert zurückgeben. Daher muss sichergestellt werden, dass die Nodes immer in der gleichen Reihenfolge vorkommen. Um die Liste der Nodes zu sortieren, müssen einzigartige Eigenschaften der Nodes verglichen werden.
Es können niemals zwei HiveMQ Nodes dieselbe \ac{ip} Adresse und den selben Port haben. Beide Werte als String vereint können verglichen werden, um bei gleichbleibenden Nodes dieselbe Reihenfolge in einer Liste einzuhalten.
\begin{figure}
    \import{gen/}{snapshot-versioning}
    \caption{Generierung einer Snapshot Version basierend auf HiveMQ Node Eigenschaften}
    \label{code:snapshot-versioning}
\end{figure}
\\
Die Funktion \verb|generateVersion| aus Quellcodeauszug \ref{code:snapshot-versioning} generiert die Version für einen Snapshot. Es wird eine HiveMQ Node Liste übergeben und ein String zurückgegeben, der als Version des Snapshots dient. Nach der Sortierung der HiveMQ Node Liste nach Identifier, der aus Hostnamen und Port besteht, werden die Werte Identifier (Hostname und Port), Gewichtung und Health Status in eine Hash Funktion gegeben. Das Ergebnis der Hash Funktion ist der Rückgabewert der Funktion \verb|generateVersion|.

\subsection{Sticky Session} \label{ss:sticky-session}
Ein HiveMQ Broker speichert temporär Daten von \ac{mqtt} Clients ab. Darunter sind Daten wie zum Beispiel Topic Aliasse oder Nachrichten, die nicht zugestellt werden konnten, weil die Verbindung des Clients unterbrochen wurde. Sobald sich ein Client, bei dem die Verbindung unterbrochen wurde, wieder mit dem Broker verbindet, kann der Broker die vorbehaltenen Nachrichten zustellen.
Dies verringert den Datenverlust in instabilen Netzwerken.
\\
Wie in Kapitel \ref{sp:persistent-session} erläutert, werden clientspezifische Daten nur auf bestimmten HiveMQ Nodes eines Clusters gespeichert.
Wenn die Verbindung eines Clients unterbrochen wird und bei der Wiederverbindung mit einem anderen Node als zuvor verbunden wird, müssen die Daten des Clients im Cluster synchronisiert werden.
Im Optimalfall verbindet sich der Client mit demselben Node wie zuvor, wodurch keine Synchronisation notwendig ist.
Ein solches Verfahren nennt sich \textit{Sticky Session} oder \textit{Session Affinity}. Dabei werden einzigartige Identifier eines Clients im Load Balancer extrahiert, und beispielsweise in einer HashMap mit dem Zielnode gespeichert. Sobald der Client eine erneute Verbindung aufbaut, kann der Load Balancer in der HashMap den alten Node nachschlagen und den Client mit diesem Node verbinden. \cite{WhatDoesTerm}

\subsubsection{Clientkennung} \label{ss:clientid}
Jeder \ac{mqtt} Client wird durch eine einzigartige Clientkennung identifiziert. Dieser wird, wie in Kapitel \ref{s:mqtt-connect} beschrieben, in dem \ac{mqtt} \verb|CONNECT| Paket beim Verbindungsaufbau vom Client an den Broker geschickt.
Eine Clientkennung darf niemals doppelt vergeben werden. Sobald sich ein Client mit dem Broker verbindet, dessen Kennung bereits von einem anderen Client genutzt wird, unterbricht der Broker die Verbindung mit dem bereits verbundenen Client und führt einen Client Takeover für den neuen Client durch. Dadurch wird sichergestellt, dass jede Clientkennung nur einmal vergeben ist.
\\
Die Clientkennung eignet sich durch ihre Einzigartigkeit als Identifier für eine Sticky Session.

\subsubsection{MQTT CONNECT}
In Kapitel \ref{ss:clientid} wurde die Benutzung der \ac{mqtt} Clientkennung als Merkmal für eine Sticky Session beschrieben. Envoy ist im Kern ein \ac{osi}-Layer vier \acl{lb} und hat auf Informationen, wie eine Layer sieben \ac{mqtt} Clientkennung, keinen Zugriff. Wie in Kapitel \ref{s:envoy} erläutert, kann Envoy zum Beispiel für \ac{http} als Layer sieben \ac{lb} betrieben werden, da das Dekodieren von \ac{http} Paketen bereits in Envoy implementiert wurde.
Das \ac{mqtt} Protokoll ist nicht in Envoy integriert. Aus diesem Grund gibt es keine Möglichkeit in der Envoy Konfiguration eine load balancing Entscheidung anhand der Clientkennung zu treffen, da Envoy die einzelnen \ac{mqtt} Pakete nicht dekodiert.
\\
Envoy ermöglicht das Einbinden von Web Assembly Modulen, um diese als Netzwerk Filter zu benutzen. Ein Netzwerk Filter kann \ac{mqtt} Pakete dekodieren, um diese den nachfolgenden Filtern in der Pipeline zur Verfügung zu stellen. Bislang gibt es \ac{wasm} \acp{sdk} für Rust, C++, AssemblyScript und Golang. \cite{sebastianHowWriteWASM} Beispiele für eine Implementation eines Golang Netzwerk Filters finden sich auf GitHub \cite{TetratelabsProxywasmgosdk2021}.
\\
Das Dekodieren der \ac{mqtt} Pakete in Golang wurde bereits von der Eclipse \ac{mqtt} Bibliothek \cite{EclipsePahoMqtt2021} implementiert.
Der Quellcodeauszug \ref{code:mqtt-connect-decode} zeigt einen Auszug aus der Datei \verb|packets/connect.go| des Repositories und dekodiert ein \ac{mqtt} \verb|CONNECT| Paket. Die Fehlerabhandlung wurde für bessere Lesbarkeit entfernt. Quellcodeauszug \ref{code:mqtt-connect-struct} zeigt die Struktur eines \verb|CONNECT| Paketes basierend der in Kapiel \ref{s:mqtt-connect} beschriebenen Felder.
\begin{figure}
    \import{gen/}{mqtt-connect-struct}
    \caption{Golang Struktur des MQTT CONNECT Paketes. Quelle: \cite{EclipsePahoMqtt2021}}
    \label{code:mqtt-connect-struct}
\end{figure}
\begin{figure}
    \import{gen/}{mqtt-connect-decode}
    \caption{Dekodieren eines MQTT CONNECT Paketes in Golang. Quelle: \cite{EclipsePahoMqtt2021}}
    \label{code:mqtt-connect-decode}
\end{figure}
\\
Bei jeder neuen Clientverbindung muss der \acl{lb} das erste Paket des Clients mit der Funktion \verb|Unpack| aus Quellcodeauszug \ref{code:mqtt-connect-decode} dekodieren um an den Wert von\newline
\verb|ConnectPacket.ClientIdentifier| zu gelangen.
Die Clientkennung wird beispielsweise in einer HashMap gespeichert, um bei neuen Clientverbindungen zu prüfen, ob der Client bereits mit einem Node verbunden wurde.

\subsubsection{Cluster Health}
Bei dem in Kapitel \ref{ss:weighted-cpu} vorgestellten weighted round-robin load balancing Algorithmus ist es nicht möglich, einen individuellen Client mit einem bestimmten Node zu verbinden. Der \acl{lb} muss entscheiden, ob ein Client basierend dem weighted round-robin oder dem Sticky Session Algorithmus aus Kapitel \ref{ss:clientid} mit einem Zielnode verbindet.
\\
Eine Möglichkeit, beide Algorithmen in einem \acl{lb} zu verwenden, ist bei einem neuen Client zuerst in der HashMap nachzuschlagen, ob dieser Client bereits mit einem Node verbunden wurde. Falls dieser Node noch vorhanden und \verb|HEALTHY| ist, wird der Client erneut mit dem Node verbunden. Andernfalls wird ein neuer Node basierend dem weighted round-robin Algorithmus bestimmt.
Diese Methode harmoniert mit dem beschriebenen Überlastschutz aus Kapitel \ref{ss:circuit-breaking}. Für den Fall, dass ein HiveMQ Node die Verbindung eines Clients unterbricht, weil der Node überlastet ist, markiert der Überlastschutz diesen Node als \verb|UNHEALTHY| und der \acl{lb} wird den Client, der sich wieder mit dem Cluster verbindet, mit einem anderen Node verbinden.
\\
Durch diese Implementation der Sticky Session werden Clients, die ihre Verbindung verlieren, immer mit demselben Node wiederverbunden und Clients, deren Verbindung absichtlich beendet wurde, werden einem neuen Node zugewiesen. Clients, die sich zum ersten Mal mit dem Cluster verbinden, werden basierend der freien Last der einzelnen Nodes verteilt.
\newpage

\begin{comment}
- This is supposed to be the core of your thesis or project. Describe your work from a con-ceptual viewpoint.
- Example: In case you have developed some prototypical tool in your bachelor thesis, demonstrate how it is employed in its business context. More concrete example: Assume that your contribution is a Maven-Build-Plugin that further automates the deployment of changes to the claim handling process into production. In this case show how the plugin is integrated in the overall (continuous) integration and deployment process, which human ac-tors are involved, which external systems and so on. Elaborate on subtle edge cases you had to deal with, e.g., possible outages of external systems.
- Usedi  agramswhere appropriate. Standard notations are better than informal box-and-line-diagrams. Typical standard notations for a solution concept are
  - Business Process Modelling Notation (BPMN) or UML activity diagrams, that depict a workflow in which your tool is used
  - UML component diagrams, where your tool is represented by just a single component (without its ingredients) together with connected external systems
\end{comment}
